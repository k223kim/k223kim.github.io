[{"content":"Commit Convention Overview 1 2 3 4 5 \u0026lt;type\u0026gt;[optional scope]: \u0026lt;description\u0026gt; [blank] [optional body] [blank] [optional footer(s)] Type fix : bug fix feat : new feature etc build : changes that affect build system/external dependencies chore : updating grunt tasks (no production code change) ci : changes to CI configuration style : changes to the code style only (white-space, format, semic-colons, etc) refactor : not fix nor feat perf : improves performance test : adding/correcting tests docs : documentation change ! after type / scope draws attention (obviously) Footer BREAKING CHANGE breaking API change for any type/scope Signed-off-by Reviewed-by Helped-by Reference-to See-also Examples 1 fix: updated so that the API only takes valid uris 1 2 3 4 5 feat!: constantly checks for new ground truth dataset in the DB and trains the model BREAKING CHANGE: airflow DAG created to run this feature weekly Reviewed by: Kaeun Kim Branch Conventions Main Branches master : production-ready develop : latest development Supporting Branches feature branch off from develop when developing new features merge back to develop to add the new feature in the new release Naming convention: anything other than other branches convention release branch off from develop when develop (almost) reflects the desired state of the new release 1 git checkout -b release-1.1 develop merge back to \u0026hellip; master when it is ready to become a real release 1 2 3 4 5 6 7 git checkout master git merge --no-f release-1.1 git tag -a 1.1``` - `develop` for further development of the next release ```bash git checkout develop git merge --no-f release-1.1 Naming convention: release-** hotfix branch off from master when it is necessary to act immediately upon an undesired state of a live production version 1 git checkout -b hotfix-1.0.1 master merge back to \u0026hellip; master when the bug fix is done 1 2 3 git checkout master git merge --no-ff hotfix-1.0.1 git tag -a 1.0.1 develop to safeguard the bug fix for the next release 1 2 git checkout develop git merge --no-f release-1.1 Naming convention: hotfix-** References https://github.com/angular/angular/blob/22b96b9/CONTRIBUTING.md#type https://www.conventionalcommits.org/en/v1.0.0/ https://nvie.com/posts/a-successful-git-branching-model/ ","date":"2023-12-04T00:08:07+09:00","permalink":"https://k223kim.github.io/p/git-conventions/","title":"Git Conventions"},{"content":"Stored Procedure Basics 1 2 3 4 CREATE PROCEDURE my_procedure() ([Parameter 1], [Parameter 2]) BEGIN SQL Queries END; Stored procedure is a collection of pre-compiled SQL statements wrapped within a CREATE PROCEDURE statement. Execution of a SQL query consists of the following steps (details): Compiling \u0026amp; Binding → Optimizing → Executing Compiling Parsing, syntax check, semantics check Convert to binary representation (execution plan) Optimizing Choose algorithms to use (optimize the execution plan) Executing Run query and fetch output As stored procedure is pre-compiled (already compiled on the server side(DB); first step of execution is done), it is faster than a MySQL function (this runs on the client; please see here for further details regarding the difference between a procedure and a function). Can be called as shown below: 1 call my_procedure() Stored Procedure Parameters Three modes for MySQL stored procedures: IN, OUT, and INOUT. IN: input parameters OUT: output parameters (returned parameters) INOUT: output parameter that is determined by the input parameter We must be also provide the datatype and its length. Below are some examples: IN parameter 1 2 3 4 CREATE PROCEDURE first_procedure(IN input_parameter varchar(50)) BEGIN ... END; 1 CALL first_procedure(\u0026#39;hello\u0026#39;) OUT parameter 1 2 3 4 CREATE PROCEDURE second_procedure(OUT returned_value int) BEGIN ... END; 1 2 CALL second_procedure(@val) SELECT @val as Output INOUT parameter 1 2 3 4 CREATE PROCEDURE third_procedure(INOUT returned_value int, IN input varchar(10)) BEGIN ... END; 1 2 CALL third_procedure(@out, \u0026#39;kaeun\u0026#39;); SELECT @out as Output; Viewing Stored Procedure 1 SHOW CREATE PROCEDURE my_procedure; Stored Procedure with JSON Let us consider the case where the input parameter is a JSON file. Consider the following json file.\n1 2 3 4 5 { \u0026#39;name\u0026#39;: \u0026#39;Kaeun Kim\u0026#39;, \u0026#39;age\u0026#39;: 27, \u0026#39;pets\u0026#39;: [\u0026#39;tico\u0026#39;, \u0026#39;hope\u0026#39;] } Also, consider a table info that has the following columns: name, age, pet. I want to create a stored procedure to filter out rows that has either the same pet as the input JSON, older than the input age.\n1 2 3 | name | age | pet | |------|-----|-----| | | | | This can be achieved by the following stored procedure my_procedure.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 CREATE PROCEDURE my_procedure(json_info JSON) BEGIN SET @name = JSON_UNQUOTE(JSON_VALUE(json_info, \u0026#39;$.name\u0026#39;)) SET @age = JSON_VALUE(json_info, \u0026#39;$.age\u0026#39;) SET @pets = NULLIF(JSON_UNQUOTE(JSON_EXTRACT(json_info, \u0026#39;$.pets\u0026#39;)), \u0026#39;null\u0026#39;) SELECT DISTINCT info.name as name, info.age as age, info.pet as pet FROM info WHERE (@pets IS NULL OR images.pet MEMBER OF (@pets)) AND (@age IS NULL OR images.age \u0026gt; @age)\tEND; A possible output would be the following:\n1 2 3 4 5 | name | age | pet | |-------|-----|------| | John | 30 | None | | Peter | 28 | tico | | Tom | 29 | hope | Extracting Values from JSON Extract scalar value from JSON 1 JSON_VALUE(json_info, \u0026#39;$.name\u0026#39;) Extract any type of value from JSON 1 JSON_EXTRACT(json_info, \u0026#39;$.pets\u0026#39;) Variables (details) User-defined variables (i.e. session variable see here for details) Specific to the current client connection to the MySQL server. 1 SET @val = 10; Local Variables (Procedure variable) 1 2 3 4 5 6 7 8 9 10 11 DELIMITER // CREATE PROCEDURE my_procedure (input INT) BEGIN DECLARE val INT; SET val = 29; SELECT val; END; // DELIMITER ; Reinitialized to NULL each time the procedure is called References https://www.sqlshack.com/learn-mysql-the-basics-of-mysql-stored-procedures/ https://blog.duveen.me/12 ","date":"2023-11-30T10:12:24+09:00","permalink":"https://k223kim.github.io/p/stored-procedure/","title":"Stored Procedure"},{"content":"FCOS (Fully Convolutional One Stage Object Detection) The aim of this post is to fill the gap between the connectivity of the original paper and the code implementation as much as possible to fully understand the practical aspect of FCOS.\nThe key takeaways and strength of FCOS compared to other detector models are the following:\nIt is not a anchor based detector. It is a anchor-free/proposal free detector. There exists three branches; classification branch, regression branch, and center-ness branch. \u0008Because of that, its loss function is composed of the classification loss, regression loss, and a center-ness loss. FCOS Architecture FCOS Overview 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class GeneralizedRCNN(nn.Module): def forward(self, images, targets=None): images = to_image_list(images) features = self.backbone(images.tensors) proposals, proposal_losses = self.rpn(images, features, targets) if self.roi_heads: x, result, detector_losses = self.roi_heads(features, proposals, targets) else: # RPN-only models don\u0026#39;t have roi_heads x = features result = proposals detector_losses = {} if self.training: losses = {} losses.update(detector_losses) losses.update(proposal_losses) return losses return result FCOS architecture has three components:\nBackbone Input: images Output: features RPN Input : images, features, targets Output : proposals, proposal losses Usage : uses the image features to extract proposals Head Input: features, proposals, targets Output: (detection) result, detector losses Usage: uses the features from the backbone and the proposals from the RPN to get the final detection/segmentation output FCOS Module This is the main component of FCOS RPN that computes the proposals and proposal losses. Notice that to calculate the proposal losses, we need to compute the locations. This will be further discussed below when we talk about anchor-free detectors.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class FCOSModule(torch.nn.Module): \u0026#34;\u0026#34;\u0026#34; Module for FCOS computation. Takes feature maps from the backbone and FCOS outputs and losses. Only Test on FPN now. \u0026#34;\u0026#34;\u0026#34; def __init__(self, cfg, in_channels): super(FCOSModule, self).__init__() head = FCOSHead(cfg, in_channels) box_selector_test = make_fcos_postprocessor(cfg) loss_evaluator = make_fcos_loss_evaluator(cfg) self.head = head self.box_selector_test = box_selector_test self.loss_evaluator = loss_evaluator def forward(self, images, features, targets=None): box_cls, box_regression, centerness = self.head(features) locations = self.compute_locations(features) # computes the locations for each feature level if self.training: return self._forward_train( # calculates the training loss locations, box_cls, box_regression, centerness, targets ) else: return self._forward_test( # selects which box to use locations, box_cls, box_regression, centerness, images.image_sizes ) FCOS Head For clarity, please refer the following figure to understand each component of FCOS Head. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class FCOSHead(torch.nn.Module): def __init__(self, cfg, in_channels): super(FCOSHead, self).__init__() num_classes = cfg.MODEL.FCOS.NUM_CLASSES - 1 self.fpn_strides = cfg.MODEL.FCOS.FPN_STRIDES cls_tower = [] bbox_tower = [] \u0026#34;\u0026#34;\u0026#34; \u0008cls_tower = 4 conv layers bbox_tower = 4 conv layers \u0026#34;\u0026#34;\u0026#34; # initialize the bias for focal loss prior_prob = cfg.MODEL.FCOS.PRIOR_PROB bias_value = -math.log((1 - prior_prob) / prior_prob) torch.nn.init.constant_(self.cls_logits.bias, bias_value) self.scales = nn.ModuleList([Scale(init_value=1.0) for _ in range(5)]) # this refers to def forward(self, x): logits = [] bbox_reg = [] centerness = [] for l, feature in enumerate(x): cls_tower = self.cls_tower(feature) box_tower = self.bbox_tower(feature) logits.append(self.cls_logits(cls_tower)) if self.centerness_on_reg: centerness.append(self.centerness(box_tower)) else: centerness.append(self.centerness(cls_tower)) bbox_pred = self.scales[l](self.bbox_pred(box_tower)) if self.norm_reg_targets: bbox_pred = F.relu(bbox_pred) if self.training: bbox_reg.append(bbox_pred) else: bbox_reg.append(bbox_pred * self.fpn_strides[l]) else: bbox_reg.append(torch.exp(bbox_pred)) return logits, bbox_reg, centerness Notice that cls_tower and bbox_tower refers to the classification branch and regression branch from the original paper. Also, notice that the input x is a list of features on 5 different levels ($P_3$, $P_4$, $P_5$, $P_6$, $P_7$). This means that on each feature level, different bounding box regression predictions, center-ness, and logits are calculated. Note that the regression predictions on each feature level are literary in a feature-level. Additionally, the regression predictions are in the $[l, t, r, b]$ format in order to compute the center-ness later on.\n1 2 3 4 5 6 7 # FCOS/fcos_core/modeling/rpn/fcos/loss.py def compute_centerness_targets(self, reg_targets): left_right = reg_targets[:, [0, 2]] top_bottom = reg_targets[:, [1, 3]] centerness = (left_right.min(dim=-1)[0] / left_right.max(dim=-1)[0]) * \\ (top_bottom.min(dim=-1)[0] / top_bottom.max(dim=-1)[0]) return torch.sqrt(centerness) However, our target bounding boxes are in pixels (e.g. in a coco-format bounding box, it will be $[x, y, w, h]$). That being said, we have to convert the gt bounding box annotations (pixel level) to a feature-level bounding box annotations and convert the bbox annotation to a $[l,t,r,b]$ format.\nOne thing to note is how when we are performing inference of FCOS, we have to multiply the FPN strides to convert the feature-level regression output to a pixel level output. See the below code block to see how FCOSHead multiplies the FPN strides during inference:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def forward(self, x): logits = [] bbox_reg = [] centerness = [] for l, feature in enumerate(x): cls_tower = self.cls_tower(feature) box_tower = self.bbox_tower(feature) logits.append(self.cls_logits(cls_tower)) if self.centerness_on_reg: centerness.append(self.centerness(box_tower)) else: centerness.append(self.centerness(cls_tower)) bbox_pred = self.scales[l](self.bbox_pred(box_tower)) if self.norm_reg_targets: bbox_pred = F.relu(bbox_pred) if self.training: bbox_reg.append(bbox_pred) else: # during inferen bbox_reg.append(bbox_pred * self.fpn_strides[l]) else: bbox_reg.append(torch.exp(bbox_pred)) return logits, bbox_reg, centerness What does it mean to have a anchor-free detector? Anchor boxes can be viewed as pre-defined sliding windows or proposals, which are classified as positive or negative patches, with an extra offsets regression to refine the prediction of bounding box locations. \u0026hellip; (Anchor based detectors) often employ intersection over union (IOU) between anchor boxes and ground-truth boxes to determine the label of an anchor box.\nThe most common example of an anchor-based detector is Faster R-CNN. The following code block shows how Faster R-CNN uses anchors to calculate the (bounding box) regression loss. In order to calculate the regression loss, we have to figure out the regression target from the target bounding boxes and predefined anchors. See below how IOU (Intersection Over Union) is calculated for each target bounding box and anchor pair to get regression target along with their labels (whether it is an object or not/background or foreground).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # simplified code block from FCOS/fcos_core/modeling/rpn/loss.py def match_targets_to_anchors(self, anchor, target, copied_fields=[]): match_quality_matrix = boxlist_iou(target, anchor) matched_idxs = self.proposal_matcher(match_quality_matrix) # RPN doesn\u0026#39;t need any fields from target # for creating the labels, so clear them all target = target.copy_with_fields(copied_fields) # get the targets corresponding GT for each anchor # NB: need to clamp the indices because we can have a single # GT in the image, and matched_idxs can be -2, which goes # out of bounds matched_targets = target[matched_idxs.clamp(min=0)] matched_targets.add_field(\u0026#34;matched_idxs\u0026#34;, matched_idxs) return matched_targets def prepare_targets(self, anchors, targets): labels = [] regression_targets = [] for anchors_per_image, targets_per_image in zip(anchors, targets): # calculates iou between target bounding boxes and anchors # returns targets that matches the anchors matched_targets = self.match_targets_to_anchors( anchors_per_image, targets_per_image, self.copied_fields ) matched_idxs = matched_targets.get_field(\u0026#34;matched_idxs\u0026#34;) labels_per_image = self.generate_labels_func(matched_targets) labels_per_image = labels_per_image.to(dtype=torch.float32) # Background (negative examples) bg_indices = matched_idxs == Matcher.BELOW_LOW_THRESHOLD labels_per_image[bg_indices] = 0 # discard anchors that go out of the boundaries of the image if \u0026#34;not_visibility\u0026#34; in self.discard_cases: labels_per_image[~anchors_per_image.get_field(\u0026#34;visibility\u0026#34;)] = -1 # discard indices that are between thresholds if \u0026#34;between_thresholds\u0026#34; in self.discard_cases: inds_to_discard = matched_idxs == Matcher.BETWEEN_THRESHOLDS labels_per_image[inds_to_discard] = -1 # compute regression targets regression_targets_per_image = self.box_coder.encode( matched_targets.bbox, anchors_per_image.bbox ) labels.append(labels_per_image) regression_targets.append(regression_targets_per_image) return labels, regression_targets On the other hand, FCOS uses the following code block to convert the target bounding boxes to compute the regression target by calculating locations\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def prepare_targets(self, points, targets): object_sizes_of_interest = [ [-1, 64], [64, 128], [128, 256], [256, 512], [512, INF], ] expanded_object_sizes_of_interest = [] for l, points_per_level in enumerate(points): object_sizes_of_interest_per_level = \\ points_per_level.new_tensor(object_sizes_of_interest[l]) expanded_object_sizes_of_interest.append( object_sizes_of_interest_per_level[None].expand(len(points_per_level), -1) ) expanded_object_sizes_of_interest = torch.cat(expanded_object_sizes_of_interest, dim=0) num_points_per_level = [len(points_per_level) for points_per_level in points] self.num_points_per_level = num_points_per_level points_all_level = torch.cat(points, dim=0) labels, reg_targets = self.compute_targets_for_locations( points_all_level, targets, expanded_object_sizes_of_interest ) for i in range(len(labels)): labels[i] = torch.split(labels[i], num_points_per_level, dim=0) reg_targets[i] = torch.split(reg_targets[i], num_points_per_level, dim=0) labels_level_first = [] reg_targets_level_first = [] for level in range(len(points)): labels_level_first.append( torch.cat([labels_per_im[level] for labels_per_im in labels], dim=0) ) reg_targets_per_level = torch.cat([ reg_targets_per_im[level] for reg_targets_per_im in reg_targets ], dim=0) if self.norm_reg_targets: reg_targets_per_level = reg_targets_per_level / self.fpn_strides[level] reg_targets_level_first.append(reg_targets_per_level) return labels_level_first, reg_targets_level_first Personal Notes I got a chance to make use of the FCOS (specifically, CondInst and BoxInst). I\u0026rsquo;ve written what I wanted to note during different experiments and implementations down below.\nAdding new heads In order to use FCOS to calculate something else (other than detection or classification), using the bounding box features (or the classification feature, or maybe both), adding another head can be helpful. That means something like the following (+I realized how a \u0026lsquo;head\u0026rsquo; refers to a single convolutional layer.):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class FCOSHead(torch.nn.Module): def __init__(self, cfg, in_channels): super(FCOSHead, self).__init__() # original heads self.cls_logits = nn.Conv2d( in_channels, num_classes, kernel_size=3, stride=1, padding=1 ) self.bbox_pred = nn.Conv2d( in_channels, 4, kernel_size=3, stride=1, padding=1 ) self.centerness = nn.Conv2d( in_channels, 1, kernel_size=3, stride=1, padding=1 ) # adding another head self.volume_pred = nn.Conv2d(self.in_channels, 1, 3, padding=1) Feature map to pixel space (input image space) For each location $(x, y)$ on the feature map $F_i$ , we can map it back onto the input image as $(\\lfloor \\frac{s}{2}\\rfloor + xs, \\lfloor \\frac{s}{2} \\rfloor+ ys)$ which is near the center of the receptive field of the location $(x, y)$. ($s$ is the total stride until the layer)\nThis means that when the regression output is $[0.75, 0.6, 0.8, 0.8]$ at the feature level where the stride is 128, the actual predicted bounding box area is $(0.75 (left) + 0.8 (right))\\times (0.6(top) + 0.8 (bottom)) \\times 128^2 (stride)$.\nConcatenating features When concatenating input features, (here, I wanted to concatenate the classification features and bounding box regression features to make use of the class and the bounding box information all together) it can be done as the following\n1 2 3 4 5 6 7 8 ctrness_pred = self.ctrness(bbox_tower) reg_pred = self.bbox_pred(bbox_tower) logits_pred = self.cls_logits(cls_tower) reg_pred = F.relu(scale(reg_pred), inplace=True) #concatenate two features two_towers = torch.cat((bbox_tower, cls_tower), dim=1) reg_pred2 = self.another_head(two_towers) reg_pred2 = F.relu(reg_pred2, inplace=True) FCOS code structure breakdown I wanted to understand the original repository\u0026rsquo;s code structure (based on maskrcnn-benchmark\u0026rsquo;s structure). Some comments are added to the code structure.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 . ├── configs # configs for each variation of model architecture ├── demo ├── docker ├── fcos │ ├── __init__.py │ ├── bin │ ├── configs -\u0026gt; ../configs/fcos │ └── fcos.py #main FCOS ├── fcos_core │ ├── README.md │ ├── __init__.py │ ├── config │ │ ├── __init__.py │ │ ├── defaults.py │ │ └── paths_catalog.py │ ├── csrc │ ├── data # handling dataset (e.g. dataset/loader, batch sampler, collate..) │ │ ├── __init__.py │ │ ├── build.py │ │ ├── collate_batch.py │ │ ├── datasets │ │ ├── samplers │ │ └── transforms │ ├── engine │ │ ├── __init__.py │ │ ├── bbox_aug.py │ │ ├── inference.py │ │ └── trainer.py │ ├── layers │ │ ├── __init__.py │ │ ├── _utils.py │ │ ├── batch_norm.py │ │ ├── dcn │ │ ├── iou_loss.py │ │ ├── misc.py │ │ ├── nms.py │ │ ├── roi_align.py │ │ ├── roi_pool.py │ │ ├── scale.py │ │ ├── sigmoid_focal_loss.py │ │ └── smooth_l1_loss.py │ ├── modeling # different components of the architecture │ │ ├── __init__.py │ │ ├── backbone # different backbones to extract features │ │ ├── detector │ │ │ ├── __init__.py │ │ │ ├── detectors.py │ │ │ └── generalized_rcnn.py │ │ ├── roi_heads # different heads │ │ │ ├── __init__.py │ │ │ ├── box_head │ │ │ ├── keypoint_head │ │ │ ├── mask_head │ │ │ └── roi_heads.py │ │ ├── rpn # different Region Proposal Networks │ │ │ ├── __init__.py │ │ │ ├── anchor_generator.py │ │ │ ├── fcos │ │ │ │ ├── __init__.py │ │ │ │ ├── fcos.py # has FCOS module and head │ │ │ │ ├── inference.py │ │ │ │ └── loss.py │ │ │ ├── inference.py │ │ │ ├── loss.py │ │ │ ├── retinanet │ │ │ │ ├── __init__.py │ │ │ │ ├── inference.py │ │ │ │ ├── loss.py │ │ │ │ └── retinanet.py │ │ │ ├── rpn.py # this has build_rpn() to call different rpns │ │ │ └── utils.py │ │ └── utils.py │ ├── solver │ ├── structures │ └── utils ├── onnx ├── requirements.txt ├── setup.py ├── tests └── tools ","date":"2023-11-17T14:31:50+09:00","permalink":"https://k223kim.github.io/p/fcos/","title":"FCOS"},{"content":" View process and GPU status 1 nvidia-smi Information of the process 1 ps -ef | grep ${PID number} ","date":"2023-10-25T23:42:52+09:00","permalink":"https://k223kim.github.io/p/linux-commands/","title":"Linux Commands"},{"content":"I often use coco format detection/segmentation dataset and want to note some useful commands. The library used for coco dataset is called pycocotools. Below are some common usage of pycocotools (for me at least).\n1 from pycocotools import coco Useage Loading coco dataset 1 2 coco_path = \u0026#39;/path/to/coco.json\u0026#39; coco_info = coco.COCO(coco_path) Loading images and appropriate annotations 1 2 3 imgs = coco_info.loadImgs(coco_info.getImgIds()) #all images in the dataset for img in imgs: anns = coco_info.loadAnns(coco_info.getAnnIds(imgIds=img[\u0026#39;id\u0026#39;])) Loading annotations based on categories 1 2 3 4 category_names = [\u0026#39;cls1\u0026#39;, \u0026#39;cls2\u0026#39;, \u0026#39;cls3\u0026#39;, \u0026#39;cls4\u0026#39;] thsc_ids = coco_info.getCatIds(catNms=category_names) ann_ids = coco_info.getAnnIds(catIds=thsc_ids) ann_info = coco_info.loadAnns(ann_ids) Overwriting the coco dataset and save to json 1 2 3 coco_info.dataset[\u0026#39;images\u0026#39;] = new_images # list of new images with open(\u0026#39;/path/to/new/coco.json\u0026#39;, \u0026#39;w+\u0026#39;) as f: json.dump(coco_info.dataset, f) ","date":"2023-10-25T21:57:42+09:00","permalink":"https://k223kim.github.io/p/coco-dataset-and-pycocotools/","title":"COCO dataset and pycocotools"},{"content":"Often times, you may have multiple github accounts in one Mac. Below are the steps to add ssh-key for multiple github accounts.\nGenerate SSH keys You must generate ssh keys for each github account. Make sure to have unique names for each key file. Notice that the email used for the ssh key generation is irrelevant to the github account. (In fact, as per this source, email is just a comment?)\n1 ssh-keygen -t rsa -b 4096 -C \u0026#34;your_email@example.com\u0026#34; When prompted: Enter file in which to save the key, make sure to modify the name for each account (e.g. id_rsa_kaeun).\nAdd SSH key to Github Account Go to Settings/SSH and GPG Keys in your Github Account and add copy and paste the output of the following command.\n1 cat ~/.ssh/\u0026lt;ssh-key file name\u0026gt;.pub Modify ~/.ssh/config Add the following to your ~/.ssh/config file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 #user1 account Host github.com-user1 HostName github.com User git IdentityFile ~/.ssh/github-user1 IdentitiesOnly yes #user2 account Host github.com-user2 HostName github.com User git IdentityFile ~/.ssh/github-user2 IdentitiesOnly yes SSH agent 1 2 3 # ssh-add \u0026lt;IdentityFile\u0026gt; ssh-add **~/.ssh/${name of ssh key file}** # Use **eval \u0026#34;$(ssh-agent -s)\u0026#34;** if necessary Clone When cloning repositories run the following command.\n1 git clone git@${Host}:${user_name}/${your-repo-name}.git Delete recently pushed commit 1 2 git reset HEAD^ git push -f origin ${branch_name} References https://gist.github.com/Jonalogy/54091c98946cfe4f8cdab2bea79430f9 ","date":"2023-10-25T18:03:00+09:00","image":"https://k223kim.github.io/p/github-ssh-set-up/GitHub-Mark_hu9d76232819f01fb0167220a8ea045d21_7249_120x120_fill_box_smart1_3.png","permalink":"https://k223kim.github.io/p/github-ssh-set-up/","title":"Github Ssh Set Up"}]