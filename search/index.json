[{"content":"FCOS (Fully Convolutional One Stage Object Detection) The aim of this post is to fill the gap between the connectivity of the original paper and the code implementation as much as possible to fully understand the practical aspect of FCOS.\nThe key takeaways and strength of FCOS compared to other detector models are the following:\nIt is not a anchor based detector. It is a anchor-free/proposal free detector. There exists three branches; classification branch, regression branch, and center-ness branch. \u0008Because of that, its loss function is composed of the classification loss, regression loss, and a center-ness loss. FCOS Architecture FCOS Overview 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class GeneralizedRCNN(nn.Module): def forward(self, images, targets=None): images = to_image_list(images) features = self.backbone(images.tensors) proposals, proposal_losses = self.rpn(images, features, targets) if self.roi_heads: x, result, detector_losses = self.roi_heads(features, proposals, targets) else: # RPN-only models don\u0026#39;t have roi_heads x = features result = proposals detector_losses = {} if self.training: losses = {} losses.update(detector_losses) losses.update(proposal_losses) return losses return result FCOS architecture has three components:\nBackbone Input: images Output: features RPN Input : images, features, targets Output : proposals, proposal losses Usage : uses the image features to extract proposals Head Input: features, proposals, targets Output: (detection) result, detector losses Usage: uses the features from the backbone and the proposals from the RPN to get the final detection/segmentation output FCOS Module This is the main component of FCOS RPN that computes the proposals and proposal losses. Notice that to calculate the proposal losses, we need to compute the locations. This will be further discussed below when we talk about anchor-free detectors.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class FCOSModule(torch.nn.Module): \u0026#34;\u0026#34;\u0026#34; Module for FCOS computation. Takes feature maps from the backbone and FCOS outputs and losses. Only Test on FPN now. \u0026#34;\u0026#34;\u0026#34; def __init__(self, cfg, in_channels): super(FCOSModule, self).__init__() head = FCOSHead(cfg, in_channels) box_selector_test = make_fcos_postprocessor(cfg) loss_evaluator = make_fcos_loss_evaluator(cfg) self.head = head self.box_selector_test = box_selector_test self.loss_evaluator = loss_evaluator def forward(self, images, features, targets=None): box_cls, box_regression, centerness = self.head(features) locations = self.compute_locations(features) # computes the locations for each feature level if self.training: return self._forward_train( # calculates the training loss locations, box_cls, box_regression, centerness, targets ) else: return self._forward_test( # selects which box to use locations, box_cls, box_regression, centerness, images.image_sizes ) FCOS Head For clarity, please refer the following figure to understand each component of FCOS Head. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class FCOSHead(torch.nn.Module): def __init__(self, cfg, in_channels): super(FCOSHead, self).__init__() num_classes = cfg.MODEL.FCOS.NUM_CLASSES - 1 self.fpn_strides = cfg.MODEL.FCOS.FPN_STRIDES cls_tower = [] bbox_tower = [] \u0026#34;\u0026#34;\u0026#34; \u0008cls_tower = 4 conv layers bbox_tower = 4 conv layers \u0026#34;\u0026#34;\u0026#34; # initialize the bias for focal loss prior_prob = cfg.MODEL.FCOS.PRIOR_PROB bias_value = -math.log((1 - prior_prob) / prior_prob) torch.nn.init.constant_(self.cls_logits.bias, bias_value) self.scales = nn.ModuleList([Scale(init_value=1.0) for _ in range(5)]) # this refers to def forward(self, x): logits = [] bbox_reg = [] centerness = [] for l, feature in enumerate(x): cls_tower = self.cls_tower(feature) box_tower = self.bbox_tower(feature) logits.append(self.cls_logits(cls_tower)) if self.centerness_on_reg: centerness.append(self.centerness(box_tower)) else: centerness.append(self.centerness(cls_tower)) bbox_pred = self.scales[l](self.bbox_pred(box_tower)) if self.norm_reg_targets: bbox_pred = F.relu(bbox_pred) if self.training: bbox_reg.append(bbox_pred) else: bbox_reg.append(bbox_pred * self.fpn_strides[l]) else: bbox_reg.append(torch.exp(bbox_pred)) return logits, bbox_reg, centerness Notice that cls_tower and bbox_tower refers to the classification branch and regression branch from the original paper. Also, notice that the input x is a list of features on 5 different levels ($P_3$, $P_4$, $P_5$, $P_6$, $P_7$). This means that on each feature level, different bounding box regression predictions, center-ness, and logits are calculated. Note that the regression predictions on each feature level are literary in a feature-level. Additionally, the regression predictions are in the $[l, t, r, b]$ format in order to compute the center-ness later on.\n1 2 3 4 5 6 7 # FCOS/fcos_core/modeling/rpn/fcos/loss.py def compute_centerness_targets(self, reg_targets): left_right = reg_targets[:, [0, 2]] top_bottom = reg_targets[:, [1, 3]] centerness = (left_right.min(dim=-1)[0] / left_right.max(dim=-1)[0]) * \\ (top_bottom.min(dim=-1)[0] / top_bottom.max(dim=-1)[0]) return torch.sqrt(centerness) However, our target bounding boxes are in pixels (e.g. in a coco-format bounding box, it will be $[x, y, w, h]$). That being said, we have to convert the gt bounding box annotations (pixel level) to a feature-level bounding box annotations and convert the bbox annotation to a $[l,t,r,b]$ format.\nOne thing to note is how when we are performing inference of FCOS, we have to multiply the FPN strides to convert the feature-level regression output to a pixel level output. See the below code block to see how FCOSHead multiplies the FPN strides during inference:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def forward(self, x): logits = [] bbox_reg = [] centerness = [] for l, feature in enumerate(x): cls_tower = self.cls_tower(feature) box_tower = self.bbox_tower(feature) logits.append(self.cls_logits(cls_tower)) if self.centerness_on_reg: centerness.append(self.centerness(box_tower)) else: centerness.append(self.centerness(cls_tower)) bbox_pred = self.scales[l](self.bbox_pred(box_tower)) if self.norm_reg_targets: bbox_pred = F.relu(bbox_pred) if self.training: bbox_reg.append(bbox_pred) else: # during inferen bbox_reg.append(bbox_pred * self.fpn_strides[l]) else: bbox_reg.append(torch.exp(bbox_pred)) return logits, bbox_reg, centerness What does it mean to have a anchor-free detector? Anchor boxes can be viewed as pre-defined sliding windows or proposals, which are classified as positive or negative patches, with an extra offsets regression to refine the prediction of bounding box locations. \u0026hellip; (Anchor based detectors) often employ intersection over union (IOU) between anchor boxes and ground-truth boxes to determine the label of an anchor box.\nThe most common example of an anchor-based detector is Faster R-CNN. The following code block shows how Faster R-CNN uses anchors to calculate the (bounding box) regression loss. In order to calculate the regression loss, we have to figure out the regression target from the target bounding boxes and predefined anchors. See below how IOU (Intersection Over Union) is calculated for each target bounding box and anchor pair to get regression target along with their labels (whether it is an object or not/background or foreground).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # simplified code block from FCOS/fcos_core/modeling/rpn/loss.py def match_targets_to_anchors(self, anchor, target, copied_fields=[]): match_quality_matrix = boxlist_iou(target, anchor) matched_idxs = self.proposal_matcher(match_quality_matrix) # RPN doesn\u0026#39;t need any fields from target # for creating the labels, so clear them all target = target.copy_with_fields(copied_fields) # get the targets corresponding GT for each anchor # NB: need to clamp the indices because we can have a single # GT in the image, and matched_idxs can be -2, which goes # out of bounds matched_targets = target[matched_idxs.clamp(min=0)] matched_targets.add_field(\u0026#34;matched_idxs\u0026#34;, matched_idxs) return matched_targets def prepare_targets(self, anchors, targets): labels = [] regression_targets = [] for anchors_per_image, targets_per_image in zip(anchors, targets): # calculates iou between target bounding boxes and anchors # returns targets that matches the anchors matched_targets = self.match_targets_to_anchors( anchors_per_image, targets_per_image, self.copied_fields ) matched_idxs = matched_targets.get_field(\u0026#34;matched_idxs\u0026#34;) labels_per_image = self.generate_labels_func(matched_targets) labels_per_image = labels_per_image.to(dtype=torch.float32) # Background (negative examples) bg_indices = matched_idxs == Matcher.BELOW_LOW_THRESHOLD labels_per_image[bg_indices] = 0 # discard anchors that go out of the boundaries of the image if \u0026#34;not_visibility\u0026#34; in self.discard_cases: labels_per_image[~anchors_per_image.get_field(\u0026#34;visibility\u0026#34;)] = -1 # discard indices that are between thresholds if \u0026#34;between_thresholds\u0026#34; in self.discard_cases: inds_to_discard = matched_idxs == Matcher.BETWEEN_THRESHOLDS labels_per_image[inds_to_discard] = -1 # compute regression targets regression_targets_per_image = self.box_coder.encode( matched_targets.bbox, anchors_per_image.bbox ) labels.append(labels_per_image) regression_targets.append(regression_targets_per_image) return labels, regression_targets On the other hand, FCOS uses the following code block to convert the target bounding boxes to compute the regression target by calculating locations\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def prepare_targets(self, points, targets): object_sizes_of_interest = [ [-1, 64], [64, 128], [128, 256], [256, 512], [512, INF], ] expanded_object_sizes_of_interest = [] for l, points_per_level in enumerate(points): object_sizes_of_interest_per_level = \\ points_per_level.new_tensor(object_sizes_of_interest[l]) expanded_object_sizes_of_interest.append( object_sizes_of_interest_per_level[None].expand(len(points_per_level), -1) ) expanded_object_sizes_of_interest = torch.cat(expanded_object_sizes_of_interest, dim=0) num_points_per_level = [len(points_per_level) for points_per_level in points] self.num_points_per_level = num_points_per_level points_all_level = torch.cat(points, dim=0) labels, reg_targets = self.compute_targets_for_locations( points_all_level, targets, expanded_object_sizes_of_interest ) for i in range(len(labels)): labels[i] = torch.split(labels[i], num_points_per_level, dim=0) reg_targets[i] = torch.split(reg_targets[i], num_points_per_level, dim=0) labels_level_first = [] reg_targets_level_first = [] for level in range(len(points)): labels_level_first.append( torch.cat([labels_per_im[level] for labels_per_im in labels], dim=0) ) reg_targets_per_level = torch.cat([ reg_targets_per_im[level] for reg_targets_per_im in reg_targets ], dim=0) if self.norm_reg_targets: reg_targets_per_level = reg_targets_per_level / self.fpn_strides[level] reg_targets_level_first.append(reg_targets_per_level) return labels_level_first, reg_targets_level_first Personal Notes I got a chance to make use of the FCOS (specifically, CondInst and BoxInst). I\u0026rsquo;ve written what I wanted to note during different experiments and implementations down below.\nAdding new heads In order to use FCOS to calculate something else (other than detection or classification), using the bounding box features (or the classification feature, or maybe both), adding another head can be helpful. That means something like the following (+I realized how a \u0026lsquo;head\u0026rsquo; refers to a single convolutional layer.):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class FCOSHead(torch.nn.Module): def __init__(self, cfg, in_channels): super(FCOSHead, self).__init__() # original heads self.cls_logits = nn.Conv2d( in_channels, num_classes, kernel_size=3, stride=1, padding=1 ) self.bbox_pred = nn.Conv2d( in_channels, 4, kernel_size=3, stride=1, padding=1 ) self.centerness = nn.Conv2d( in_channels, 1, kernel_size=3, stride=1, padding=1 ) # adding another head self.volume_pred = nn.Conv2d(self.in_channels, 1, 3, padding=1) Feature map to pixel space (input image space) For each location $(x, y)$ on the feature map $F_i$ , we can map it back onto the input image as $(\\lfloor \\frac{s}{2}\\rfloor + xs, \\lfloor \\frac{s}{2} \\rfloor+ ys)$ which is near the center of the receptive field of the location $(x, y)$. ($s$ is the total stride until the layer)\nThis means that when the regression output is $[0.75, 0.6, 0.8, 0.8]$ at the feature level where the stride is 128, the actual predicted bounding box area is $(0.75 (left) + 0.8 (right))\\times (0.6(top) + 0.8 (bottom)) \\times 128^2 (stride)$.\nConcatenating features When concatenating input features, (here, I wanted to concatenate the classification features and bounding box regression features to make use of the class and the bounding box information all together) it can be done as the following\n1 2 3 4 5 6 7 8 ctrness_pred = self.ctrness(bbox_tower) reg_pred = self.bbox_pred(bbox_tower) logits_pred = self.cls_logits(cls_tower) reg_pred = F.relu(scale(reg_pred), inplace=True) #concatenate two features two_towers = torch.cat((bbox_tower, cls_tower), dim=1) reg_pred2 = self.another_head(two_towers) reg_pred2 = F.relu(reg_pred2, inplace=True) FCOS code structure breakdown I wanted to understand the original repository\u0026rsquo;s code structure (based on maskrcnn-benchmark\u0026rsquo;s structure). Some comments are added to the code structure.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 . ├── configs # configs for each variation of model architecture ├── demo ├── docker ├── fcos │ ├── __init__.py │ ├── bin │ ├── configs -\u0026gt; ../configs/fcos │ └── fcos.py #main FCOS ├── fcos_core │ ├── README.md │ ├── __init__.py │ ├── config │ │ ├── __init__.py │ │ ├── defaults.py │ │ └── paths_catalog.py │ ├── csrc │ ├── data # handling dataset (e.g. dataset/loader, batch sampler, collate..) │ │ ├── __init__.py │ │ ├── build.py │ │ ├── collate_batch.py │ │ ├── datasets │ │ ├── samplers │ │ └── transforms │ ├── engine │ │ ├── __init__.py │ │ ├── bbox_aug.py │ │ ├── inference.py │ │ └── trainer.py │ ├── layers │ │ ├── __init__.py │ │ ├── _utils.py │ │ ├── batch_norm.py │ │ ├── dcn │ │ ├── iou_loss.py │ │ ├── misc.py │ │ ├── nms.py │ │ ├── roi_align.py │ │ ├── roi_pool.py │ │ ├── scale.py │ │ ├── sigmoid_focal_loss.py │ │ └── smooth_l1_loss.py │ ├── modeling # different components of the architecture │ │ ├── __init__.py │ │ ├── backbone # different backbones to extract features │ │ ├── detector │ │ │ ├── __init__.py │ │ │ ├── detectors.py │ │ │ └── generalized_rcnn.py │ │ ├── roi_heads # different heads │ │ │ ├── __init__.py │ │ │ ├── box_head │ │ │ ├── keypoint_head │ │ │ ├── mask_head │ │ │ └── roi_heads.py │ │ ├── rpn # different Region Proposal Networks │ │ │ ├── __init__.py │ │ │ ├── anchor_generator.py │ │ │ ├── fcos │ │ │ │ ├── __init__.py │ │ │ │ ├── fcos.py # has FCOS module and head │ │ │ │ ├── inference.py │ │ │ │ └── loss.py │ │ │ ├── inference.py │ │ │ ├── loss.py │ │ │ ├── retinanet │ │ │ │ ├── __init__.py │ │ │ │ ├── inference.py │ │ │ │ ├── loss.py │ │ │ │ └── retinanet.py │ │ │ ├── rpn.py # this has build_rpn() to call different rpns │ │ │ └── utils.py │ │ └── utils.py │ ├── solver │ ├── structures │ └── utils ├── onnx ├── requirements.txt ├── setup.py ├── tests └── tools ","date":"2023-11-17T14:31:50+09:00","permalink":"https://k223kim.github.io/p/fcos/","title":"FCOS"},{"content":" View process and GPU status 1 nvidia-smi Information of the process 1 ps -ef | grep ${PID number} ","date":"2023-10-25T23:42:52+09:00","permalink":"https://k223kim.github.io/p/linux-commands/","title":"Linux Commands"},{"content":"I often use coco format detection/segmentation dataset and want to note some useful commands. The library used for coco dataset is called pycocotools. Below are some common usage of pycocotools (for me at least).\n1 from pycocotools import coco Useage Loading coco dataset 1 2 coco_path = \u0026#39;/path/to/coco.json\u0026#39; coco_info = coco.COCO(coco_path) Loading images and appropriate annotations 1 2 3 imgs = coco_info.loadImgs(coco_info.getImgIds()) #all images in the dataset for img in imgs: anns = coco_info.loadAnns(coco_info.getAnnIds(imgIds=img[\u0026#39;id\u0026#39;])) Loading annotations based on categories 1 2 3 4 category_names = [\u0026#39;cls1\u0026#39;, \u0026#39;cls2\u0026#39;, \u0026#39;cls3\u0026#39;, \u0026#39;cls4\u0026#39;] thsc_ids = coco_info.getCatIds(catNms=category_names) ann_ids = coco_info.getAnnIds(catIds=thsc_ids) ann_info = coco_info.loadAnns(ann_ids) Overwriting the coco dataset and save to json 1 2 3 coco_info.dataset[\u0026#39;images\u0026#39;] = new_images # list of new images with open(\u0026#39;/path/to/new/coco.json\u0026#39;, \u0026#39;w+\u0026#39;) as f: json.dump(coco_info.dataset, f) ","date":"2023-10-25T21:57:42+09:00","permalink":"https://k223kim.github.io/p/coco-dataset-and-pycocotools/","title":"COCO dataset and pycocotools"},{"content":"Often times, you may have multiple github accounts in one Mac. Below are the steps to add ssh-key for multiple github accounts.\nGenerate SSH keys You must generate ssh keys for each github account. Make sure to have unique names for each key file. Notice that the email used for the ssh key generation is irrelevant to the github account. (In fact, as per this source, email is just a comment?)\n1 ssh-keygen -t rsa -b 4096 -C \u0026#34;your_email@example.com\u0026#34; When prompted: Enter file in which to save the key, make sure to modify the name for each account (e.g. id_rsa_kaeun).\nAdd SSH key to Github Account Go to Settings/SSH and GPG Keys in your Github Account and add copy and paste the output of the following command.\n1 cat ~/.ssh/\u0026lt;ssh-key file name\u0026gt;.pub Modify ~/.ssh/config Add the following to your ~/.ssh/config file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 #user1 account Host github.com-user1 HostName github.com User git IdentityFile ~/.ssh/github-user1 IdentitiesOnly yes #user2 account Host github.com-user2 HostName github.com User git IdentityFile ~/.ssh/github-user2 IdentitiesOnly yes SSH agent 1 2 3 # ssh-add \u0026lt;IdentityFile\u0026gt; ssh-add **~/.ssh/${name of ssh key file}** # Use **eval \u0026#34;$(ssh-agent -s)\u0026#34;** if necessary Clone When cloning repositories run the following command.\n1 git clone git@${Host}:${user_name}/${your-repo-name}.git References https://gist.github.com/Jonalogy/54091c98946cfe4f8cdab2bea79430f9 ","date":"2023-10-25T18:03:00+09:00","image":"https://k223kim.github.io/p/github-ssh-set-up/GitHub-Mark_hu9d76232819f01fb0167220a8ea045d21_7249_120x120_fill_box_smart1_3.png","permalink":"https://k223kim.github.io/p/github-ssh-set-up/","title":"Github Ssh Set Up"}]