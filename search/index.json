[{"content":"Below are my use cases using MLfLow\nMLflow hierarchy experiments \u0026raquo; runs\nExperiment group of runs Runs each training session Adding local MLflow experiment to desired MLflow host experiment(i.e. export and import ) Install dependencies 1 pip install mlflow-export-import Find where the mlflow run files exist and launch the local mlflow server 1 2 3 4 5 6 7 8 # location of mlflow └── mlflow ├── 0 │ └── meta.yaml ├── 1234567890123456789 # this is the experiment_id │ ├── 123abc1234567890123456789 # this is the run_id │ └── meta.yaml # this file will show the experiment name └── models 1 2 # launch mlflow server mlflow server --backend-store-uri /path/to/local/mlflow When accessing http://127.0.0.1:5000, you would be able to view the web \u0008view of MLflow. Also, there will be an experiment indicated in meta.yaml\n1 2 3 4 # example of meta.yaml artifact_location: /path/to/artifact experiment_id: \u0026#39;1234567890123456789\u0026#39; name: /Shared/model Here, the name of the experiment is /Shared/model 3. Export local experiment\n1 2 export MLFLOW_TRACKING_URI=http://localhost:5000 export-experiment --experiment experiment_name --output-dir /path/to/save/exported/runs Note that the experiment_name must match the experiment name in the meta.yaml file (i.e. /Shared/model in this case). Once the export is successful, you will be able to view the individual run_ids under the directory assigned in --output-dir 4. Import the local experiment\n1 2 export MLFLOW_TRACKING_URI=/desired/mlflow/uri import-experiment --experiment-name yolo --input-dir /path/to/exported/runs Notice when importing the local experiments, a different experiment name can be assigned. In my specific example, as my models were yolo type models, I used the name yolo. For the --input-dir, it must be identical to the --output-dir assigned during the export. If the yolo experiment existed in the /desired/mlflow/uri, it will add a new run. If not, it will create a new experiment and add the exported run.\nNote The same logic is applied to export-run and import-run. Altering artifact saving location for a specific experiment 1 2 3 import mlflow mlflow.set_tracking_uri(\u0026#34;http://localhost:5000\u0026#34;) mlflow.create_experiment(\u0026#34;new_experiment\u0026#34;, \u0026#34;s3://my-bucket/\u0026#34;) Activate run To modify a run (e.g. register model, add tag, etc), that run has to be active\n1 2 3 4 5 with mlflow.start_run(run_id = run_id) as run: # do something # can access run id by run.info.run_id # make sure to end the run mlflow.end_run() MLflow Model Standard format for packaging machine learning models Can be used easily in different downstream tools (e.g. can be registered, can use built-in deployment tools, perform model evaluation) MLflow supports logging different flavors of models Some examples of supported model types PyTorch ONNX Keras Created by 1 2 3 mlflow.\u0026lt;model_flavor\u0026gt;.log_model() # example # mlflow.pytorch.log_model(\u0026#34;/path/to/model\u0026#34;) Log artifact vs Log model log_artifact() : push any type of files into MLflow models added by log_artifact are not MLflow models can add models that are not supported by MLflow (e.g. TFLite) log_model() : creates MLflow models 1 2 # log_artifact() mlflow.log_artifact(local_path=\u0026#34;/path/to/local_file.txt\u0026#34;, artifact_path=\u0026#34;config/\u0026#34;) The above line will push local_file.txt under ${run_id}/config/\nLogging ultralytics YOLO PyTorch model 1 2 3 4 5 6 7 import torch from ultralytics import YOLO model = YOLO(\u0026#34;/path/to/model.pt\u0026#34;) torchscript_model = model.export(format=\u0026#39;torchscript\u0026#39;) jit_model = torch.jit.load(\u0026#34;/path/to/model.torchscript\u0026#34;) with mlflow.start_run(run_id=run_id): mlflow.pytorch.log_model(model, artifact_path=\u0026#34;model/\u0026#34;) This will save the torchscript model to ${run_id}/model/model.pth\nMLflow Model Registry Centralized model store Can manage full lifecycle of an MLflow Model model versioning model aliasing model tagging annotations Each registered model has multiple versions (start from 1) each version model can have a tag (set_model_version_tag) can have a tag (set_registered_model_tag) Register model 1 2 with mlflow.start_run(run_id=run_id) as run: mlflow.register_model(model_uri=f\u0026#34;runs:/{run.info.run_id}/model/\u0026#34;, name=\u0026#34;new-registry\u0026#34;) Notice that the model_uri is the artifact path in which the model was saved using log_model. Including runs:/{run_id} within the model_uri allows us to record the run ID with the model in model registry. Optionally, a tag can be added to that specific version of the registered model.\n1 mlflow.register_model(model_uri=f\u0026#34;runs:/{run.info.run_id}/model/\u0026#34;, name=\u0026#34;new-registry\u0026#34;, tags={\u0026#39;labels\u0026#39;: [1, 2, 3], \u0026#39;dataset\u0026#39;:\u0026#39;/path/to/dataset/\u0026#39;}) Registered Model Tag 1 2 3 from mlflow import MlflowClient client = MlflowClient(tracking_uri=\u0026#34;http://localhost:5000/\u0026#34;) client.set_registered_model_tag(\u0026#34;new-registry\u0026#34;, \u0026#34;task\u0026#34;, \u0026#34;classification\u0026#34;) In MLflow, tags are key, value pairs. Here, \u0026rsquo;task\u0026rsquo; is the key, and \u0026lsquo;classification\u0026rsquo; is the value of that key.\nModel Version Tag 1 client.set_model_version_tag(\u0026#34;new-registry\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;validation_status\u0026#34;, \u0026#34;approved\u0026#34;) Notice that the version is passed as a string.\nCreating runs 1 2 with mlflow.start_run(experiment_id=1, run_name=\u0026#39;train\u0026#39;) as run: # this creates a run under experiment that has an experiment_id=1 Adding metrics Assume we have a list filled with the model accuracy.\n1 accuracy = [0.61, 0.77, 0.79, 0.82, 0.87] 1 2 3 with mlflow.start_run(experiment_id=1, run_name=\u0026#39;train\u0026#39;) as run: for each_accuracy in accuracy: mlflow.log_metric(\u0026#39;accuracy\u0026#39;, each_accuracy) Adding params Assume we have a key-value dictionary that stores different parameters of the training session.\n1 2 3 4 5 param = { \u0026#34;model\u0026#34;: \u0026#34;yolo\u0026#34;, \u0026#34;epochs\u0026#34;: 300, \u0026#34;batch\u0026#34;: 512 } 1 2 with mlflow.start_run(experiment_id=1, run_name=\u0026#39;train\u0026#39;) as run: mlflow.log_params(param) ","date":"2024-03-27T11:47:41+09:00","permalink":"https://k223kim.github.io/p/mlflow/","title":"Mlflow"},{"content":"Background I have extensively utilized ultralytics for some projects and wanted to note some parts that were useful. Originally, I had a .tflite YOLO model that takes an (1x640x640x3) tensor as an input. However, the images that were used to test the model had a shape of (1x1280x1280x3) which required a resize step. Additionally, the model required a normalization step which is to divide the input tensor by 255 and convert it to float. Therefore, I wanted to include the resize and normalization steps within the tflite model to avoid that hassle. Complete source code with example images and models can be found here.\nYOLO Architecture 1 2 3 4 5 6 import torch from ultralytics import YOLO yolo_model = YOLO() # this will load a default detection YOLO model # To add your specific YOLO model, do the following # model = YOLO(\u0026#39;path/to/model.pt\u0026#39;) Model Let\u0026rsquo;s break down the yolo_model.\nyolo_model is a \u0026lt;class 'ultralytics.models.yolo.model.YOLO'\u0026gt; which looks something like this: 1 2 3 4 5 6 7 8 9 YOLO( (model): DetectionModel( (model): Sequential( (0): Conv( ... ) ... ... ... YOLO Sequential torch.nn.modules.container.Sequential model architecture is often defined using Sequential that contains multiple torch Modules forward() method accepts any input and forward it to the first module it chains the outputs to inputs sequentially for each subsequent module returns the output of the last module To access the model architecture of ultralytics YOLO, it can be done like so: 1 2 3 4 5 6 7 8 9 10 11 model.model.model \u0026#34;\u0026#34;\u0026#34; Sequential( (0): Conv( (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True) (act): SiLU(inplace=True) ) ... ) \u0026#34;\u0026#34;\u0026#34; Each Module in the Sequential can be accessed using a for loop 1 2 3 4 my_sequential = model.model.model for each_module in my_sequential: print(type(each_module)) #\u0026lt;class \u0026#39;ultralytics.nn.modules.conv.Conv\u0026#39;\u0026gt; break How does the Sequential chain the outputs and inputs for each module? Each Module has a f and i field that describes the following f The layer index which passes its output to the current layer When it is -1, it means that it gets the input from the previous layer i Current layer (Module) index Some examples Consider the following architecture of YOLOv8 provided by GitHub user RangeKing. Notice that the last block denoted by Detect is the 22nd module of the entire architecture that takes input from the 15th, 18th, and 21st C2f block. This can be confirmed by the following code block. 1 2 3 4 yolo_sequential = yolo_model.model.model # access to the sequential container detect_block = yolo_sequential[-1] # access the last module print(detect_block.i) # 22 print(detect_block.f) # [15, 18, 21] Let us consider another example. Take a look at block #20 Concat. It takes input from block #9 and #19. This can also be confirmed by the following code block.\n1 2 3 concat_block = yolo_sequential[20] print(concat_block.i) # 20 print(concat_block.f) # [-1, 9] Notice as block #20 takes the output of the previous block (#19) directly as its input, f includes -1.\nWhen f something other than -1, how does the model access the specific module\u0026rsquo;s output? That is done using an attribute of ultralytics.nn.tasks.DetectionModel called save that can be accessed as shown below. 1 2 3 detection_model = yolo_mode.model print(detection_model.save) # [4, 6, 9, 12, 15, 18, 21] When carefully considering each module in save, they are the modules that are needed for the concat block (which is obvious as the concat will need additional modules than its previous module) or the detect block. To confirm, you can check the implementation of the save attribute here:\n1 save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1) # append to savelist This shows that modules are added to save only if they take inputs from other modules other than its previous module\u0026rsquo;s output.\nCreating new Module to insert Let us create a module that we want to insert into our tflite model. We have two objectives; resize and normalization. Let us create a module as shown below. This new module needs two attributes; f and i just like any other module in YOLO.\n1 2 3 4 5 6 7 class newModule(torch.nn.Module): def __init__(self): super(newModule, self).__init__() self.f = -1 self.i = 0 def forward(self, x): return x As we are inserting a module that performs a preprocessing step of the model, therefore we know that this module will be the first (or zeroth index) module of the model. That is why i = 0 and f = -1 (this means that it simply takes the input of the model).\nResize Resize to 640x640 can be done as the following\n1 x = torch.nn.functional.interpolate(x, (640, 640), mode=\u0026#39;bilinear\u0026#39;) Normalization Dividing the tensor by 255 and converting the data type to float is shown below.\n1 x = torch.div(x, 255.).to(dtype=torch.float) New Module Adding each step to forward would complete the newModule.\n1 2 3 4 5 6 7 8 9 class newModule(torch.nn.Module): def __init__(self): super(newModule, self).__init__() self.f = -1 self.i = 0 def forward(self, x): x = torch.div(x, 255.).to(dtype=torch.float) x = torch.nn.functional.interpolate(x, (640, 640), mode=\u0026#39;bilinear\u0026#39;) return x Modifying YOLO Architecture Having the new module that we want to insert, all we are left with is to insert the module to the sequential. I have simply created a new torch.nn.Sequential and added the newModule along with the other modules in YOLO Sequential using extend.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # create deep copy of the yolo model so that we can modify it import copy yolo_model_new = copy.deepcopy(yolo_model) # create a new sequential with the new module added new_module = newModule() new_sequential = torch.nn.Sequential(new_module) for module in yolo_model_new.model.model: module.i += 1 if isinstance(module.f, list): new_f = [] for each_f in module.f: if each_f == -1: new_f.append(each_f) else: new_f.append(each_f + 1) module.f = new_f # add that to the original model yolo_model_new.model.model = new_sequential.extend(yolo_model_new.model.model) Notice how f and i were incremented by 1 as we added a single module to Sequential. Also, note how -1 did not change as it indicates the previous module. Don\u0026rsquo;t forget to update the save attribute as well.\n1 2 3 4 5 6 new_save = [] for each_idx in yolo_model_new.model.save: new_save.append(each_idx + 1) # overwrite the original save attribute yolo_model_new.model.save = new_save Similarly, the module indices in the save attribute has been incremented by 1.\nExport Default Export 1 yolo_model.export(format=\u0026#39;tflite\u0026#39;) When performing a default export, the model looks something like this: Modified Export 1 yolo_model_new.export(format=\u0026#39;tflite\u0026#39;, imgsz=(1280, 1280)) With the new module inserted and assigning a specific input tensor shape while exporting, the YOLO architecture is modified as shown below.\nNotice how the input shape is now converted to 1x1280x1280x3 and there are two \u0026ldquo;blocks\u0026rdquo; that are added that normalizes and reshapes the tensor to 1x640x640x3. The rest of the model architecture is identical.\nInference TFLite Model Let us confirm that this modification has been done correctly by running an inference on the TFLite model. Below are the code to run a the TFLite model on a single image.\nSet up TensorFlow Lite Interpreter Interface 1 2 3 4 5 6 7 8 9 10 import io import tensorflow as tf import numpy as np import cv2 from datetime import datetime start_time = datetime.now() image_path = \u0026#39;/path/to/image.png\u0026#39; interpreter = tf.lite.Interpreter(model_path=\u0026#34;/path/to/model.tflite\u0026#34;) interpreter.allocate_tensors() tf.lite.Interpreter is an interface for running TensorFlow Lite models allocate_tensors() is required before any inference as TensorFlow Lite pre-plans tensor allocations to optimize inference Get Input Tensor 1 2 input_details = interpreter.get_input_details() output_details = interpreter.get_output_details() This allows us to get the desired input and output tensor information such as shape of the tensor, data type, index, etc. Here, what we are interested in is the input and output tensor shape 1 2 input_shape = input_details[0][\u0026#39;shape\u0026#39;] # array([ 1, 224, 224, 3], dtype=int32) Now we need the actual image which is going to read an image using cv2 1 2 3 4 5 6 7 8 9 image = cv2.imread(image_path) image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) image_size = image.shape[:2] image = cv2.resize(np.copy(image), (640, 640)) #to match the desired input shape image_data = np.array(image) image_data = image_data / 255. image_data = image_data[np.newaxis, ...].astype(np.float32) Finally, we have to set the value of the input tensor (Note that set_tensor copies the the data; if you don\u0026rsquo;t want to copy the data consider using tensor()) 1 interpreter.set_tensor(input_details[0][\u0026#39;index\u0026#39;], image_data) Running the Model \u0026amp; Retrieving Results 1 2 interpreter.invoke() output_data = interpreter.get_tensor(output_details[0][\u0026#39;index\u0026#39;]) Similarly, get_tensor() will copy the value of the output tensor. tensor() can be used to get the pointer to the output tensor. Comparing two tflite models Here is the example image that I am going to use which is an image of my dog and cat. As mentioned earlier, this image has a shape of 1280x1280. Below is the complete code to run inference on two different tflite models with the same input image.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import tensorflow as tf import numpy as np import cv2 import argparse def get_image(image_path): image = cv2.imread(image_path) image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) return image def load_tflite(model_path): interpreter = tf.lite.Interpreter(model_path=model_path) interpreter.allocate_tensors() input_details = interpreter.get_input_details() output_details = interpreter.get_output_details() return interpreter, input_details, output_details def run_inference(interpreter, input_details, output_details, image): image_data = image input_shape = tuple(input_details[0][\u0026#39;shape\u0026#39;][1:3]) if input_shape != image.shape[:2]: image_data = cv2.resize(np.copy(image), input_shape) # if the shape is different assume that normalization has to be done as well image_data = np.array(image_data) image_data = image_data / 255. image_data = image_data[np.newaxis, ...].astype(np.float32) interpreter.set_tensor(input_details[0][\u0026#39;index\u0026#39;], image_data) interpreter.invoke() output_data = interpreter.get_tensor(output_details[0][\u0026#39;index\u0026#39;]) return output_data if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#39;TFLite comparison\u0026#39;) parser.add_argument(\u0026#39;--model\u0026#39;, type=str, help=\u0026#34;path/to/model.tflite\u0026#34;, default=\u0026#34;/path/to/model.tflite\u0026#34;) parser.add_argument(\u0026#39;--newModel\u0026#39;, type=str, help=\u0026#34;path/to/new_model.tflite\u0026#34;, default=\u0026#34;/path/to/new_model.tflite\u0026#34;) parser.add_argument(\u0026#39;--image\u0026#39;, type=str, help=\u0026#34;path/to/image.jpg\u0026#34;, default=\u0026#34;path/to/image.jpg\u0026#34;) args = parser.parse_args() img = get_image(args.image) interpreter_original, input_details_original, output_details_original = load_tflite(args.model) interpreter_new, input_details_new, output_details_new = load_tflite(args.newModel) output_original = run_inference(interpreter_original, input_details_original, output_details_original, img) output_new = run_inference(interpreter_new, input_details_new, output_details_new, img) assert np.allclose(output_original, output_new, atol=1e-1) == True Interesting notes Why did I choose to perform bilinear interpolation for both cv2.resize and torch.nn.functional.interpolate? That is because, when training the YOLO model, the model resizes the image using torchvision.transforms.Resize which has a default interpolation mode of bilinear (see here) To be consistent with the torch model and the tflite model, I figured that the interpolate mode should match How come the outputs are not exactly the same? I suspect two things interpolate step when resizing using interpolate, it can result in slightly different results in cv2.resize and in torch.nn.functional.interpolate normalization step as we convert the tensor into float32 , there can be a small discrepancy between numpy and torch. ","date":"2024-03-19T15:43:57+09:00","permalink":"https://k223kim.github.io/p/yolo-custom-export/","title":"YOLO Custom Export"},{"content":"References NestJS official document about providers (1, 2) The source code for this page is provided here Setup Run the following commands 1 2 3 nest new provider_practice cd provider_practice nest g co cats Whenever running the terminal commands in the following paragraphs, make sure to run the local server by doing so: 1 npm run start:dev Providers Provider can be injected as a dependency Controllers handle HTTP requests and delegate more complex tasks to providers Providers perform the business logic Dependency Injection (DI) Design pattern Inversion of Control (IoC) technique (details) Design principle in which a software component is designed to receive its dependencies from an external source rather than creating them itself Implemented using a Dependency Injection container (or the IoC container) Responsible for managing the dependencies between objects Providing them to the objects that need them Uses reflection to create objects and wire them Helps to decouple components in a system Delegate instantiation of dependencies to the IoC container (NestJS runtime system) DI system has two main roles dependency consumer dependency provider IoC container (NestJS runtime system) facilitates the interaction between them How DI system works When a dependency is requested, injector checks its registry to see if there is an instance available if not, create a new instance and store it into the registry Injecting Dependency - Class Constructor Most common way to inject a dependency is to declare it in a class constructor When the IoC container instantiate this class, it determines which services or other dependencies that class need by looking at the constructor parameter types IoC container checks if the injector has any existing instance of that service If it does not exist, IoC container creates one using the Provider Adds it to the IoC container and calls the constructor 1 2 3 4 5 6 7 8 //src/app.controller.ts import { AppService } from \u0026#39;./app.service\u0026#39;; // Inject dependency in a class constructor @Controller() export class AppController{ constructor(private readonly appService: AppService){} } Injecting Dependency - @Inject() 1 2 3 4 5 6 7 8 // Inject dependency using @Inject() //src/app.controller.ts import { Controller, Get , Inject} from \u0026#39;@nestjs/common\u0026#39;; @Controller() export class AppController{ @Inject(AppService) private readonly appService: AppService; } @Injectable() This allows the class to be a provider that can be injected in different Nest components 1 2 3 4 5 6 7 8 9 //src/app.service.ts import { Injectable } from \u0026#39;@nestjs/common\u0026#39;; @Injectable() export class AppService { getHello(): string { return \u0026#39;Hello World!\u0026#39;; } } Let us illustrate the usage of @Injectable() by using the following example @Injectable() example Setup Set up the file structure under /src/ as the following 1 2 3 4 5 6 7 8 9 10 11 12 . ├── app.controller.ts ├── app.module.ts ├── app.service.ts ├── cats │ ├── cats.controller.ts │ ├── cats.service.ts │ ├── dto │ │ └── create-cat.dto.ts │ └── interfaces │ └── cat.interface.ts └── main.ts Below shows how each file should be filled 1 2 3 4 5 6 //src/cats/interfaces/cat.interface.ts export interface Cat { name: string; age: number; breed: string; } 1 2 3 4 5 6 //src/cats/dto/create-cat.dto.ts export class CreateCatDto{ name: string; age: number; breed: string; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 //src/cats/cats.service.ts import { Injectable } from \u0026#39;@nestjs/common\u0026#39;; import { Cat } from \u0026#39;./interfaces/cat.interface\u0026#39;; @Injectable()//CatsService is now a provider export class CatsService { private readonly cats: Cat[] = []; create(cat: Cat) { this.cats.push(cat); } findAll(): Cat[] { return this.cats; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 //src/cats/cats.controller.ts import { Controller, Get, Post, Body } from \u0026#39;@nestjs/common\u0026#39;; import { CreateCatDto } from \u0026#39;./dto/create-cat.dto\u0026#39;; import { CatsService } from \u0026#39;./cats.service\u0026#39;; import { Cat } from \u0026#39;./interfaces/cat.interface\u0026#39;; @Controller(\u0026#39;cats\u0026#39;) export class CatsController { constructor(private catsService: CatsService) {} //CatsService provider is injected by Nest @Post() async create(@Body() createCatDto: CreateCatDto) { this.catsService.create(createCatDto); } @Get() async findAll(): Promise\u0026lt;Cat[]\u0026gt; { return this.catsService.findAll(); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 //src/app.module.ts import { Module } from \u0026#39;@nestjs/common\u0026#39;; import { AppController } from \u0026#39;./app.controller\u0026#39;; import { AppService } from \u0026#39;./app.service\u0026#39;; import { CatsController } from \u0026#39;./cats/cats.controller\u0026#39;; import { CatsService } from \u0026#39;./cats/cats.service\u0026#39;; @Module({ imports: [], controllers: [AppController, CatsController], providers: [AppService, CatsService], }) export class AppModule {} 1 2 3 4 curl -X POST http://localhost:3000/cats -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;tico\u0026#34;, \u0026#34;age\u0026#34;: 13, \u0026#34;breed\u0026#34;: \u0026#34;maltese\u0026#34;}\u0026#39; curl -X POST http://localhost:3000/cats -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;hope\u0026#34;, \u0026#34;age\u0026#34;: 4, \u0026#34;breed\u0026#34;: \u0026#34;british_short_hair\u0026#34;}\u0026#39; curl http://localhost:3000/cats #[{\u0026#34;name\u0026#34;:\u0026#34;hope\u0026#34;,\u0026#34;age\u0026#34;:4,\u0026#34;breed\u0026#34;:\u0026#34;british_short_hair\u0026#34;},{\u0026#34;name\u0026#34;:\u0026#34;tico\u0026#34;,\u0026#34;age\u0026#34;:13,\u0026#34;breed\u0026#34;:\u0026#34;maltese\u0026#34;}] DI Fundamentals Let us understand exactly how dependency injection occurred in the above example 1 2 3 4 5 6 7 8 9 10 11 //src/app.module.ts import { Module } from \u0026#39;@nestjs/common\u0026#39;; import { CatsController } from \u0026#39;./cats/cats.controller\u0026#39;; import { CatsService } from \u0026#39;./cats/cats.service\u0026#39;; @Module({ imports: [], controllers: [AppController, CatsController], providers: [AppService, CatsService], //register CatsService provider to Nest IoC container }) export class AppModule {} cats.service.ts has @Injectable() that declares that CatsService class can be managed by the Nest IoC container (i.e. CatsService is a provider class) cats.controller.ts has CatsController that declares a dependency on the CatsService token with the constructor injection (constructor(private catsService: CatsService)) When Nest IoC container instantiates this class CatsController, it looks for any dependencies When if finds CatsService dependency, it performs a lookup on the CatsService token this returns the CatsService class Nest will then either create an instance of CatsService class or return an existing instance app.module.ts associates the token CatsService with the class CatsService from cats.service.ts (i.e. registration) Provider Registration Associating a token with the provider class this token is used when there is a request of an instance of that particular provider class (i.e. the lookup process) CatsService: Provider class CatsController: consumer (of that service; dependency; provider) To register the service with Nest, service must be added to providers array of the @Module() decorator 1 2 3 4 5 6 //src/app.module.ts @Module({ controllers: [CatsController], providers: [CatsService], }) export class AppModule {} Above is equivalent to the following construction 1 2 3 4 5 6 7 8 9 //src/app.module.ts @Module({ controllers: [CatsController], providers: [ provide: CatsService, useClass: CatsService, ], }) export class AppModule {} Provider with Inheritance - Class Constructor Consider the following services by creating three more .ts files (serviceA.ts, serviceB.ts, and baseService.ts) 1 2 3 4 5 6 7 8 9 10 //src/cats/serviceA.ts import { Injectable } from \u0026#34;@nestjs/common\u0026#34;; @Injectable() // we need this decorator because this is used in BaseService export class ServiceA{ getCat(): string{ return \u0026#39;Hello from Cat A!\u0026#39;; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 //src/cats/baseService.ts import { ServiceA } from \u0026#34;./serviceA\u0026#34;; export class BaseService{ constructor(private readonly serviceA: ServiceA){} getCat(): string{ return \u0026#39;Hello from Base Cat!\u0026#39;; } runServiceA(): string{ return this.serviceA.getCat(); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 //src/cats/serviceB.ts import { Injectable } from \u0026#34;@nestjs/common\u0026#34;; import { BaseService } from \u0026#34;./baseService\u0026#34;; import { ServiceA } from \u0026#34;./serviceA\u0026#34;; @Injectable() // we need this decorator because this is used in CatsController export class ServiceB extends BaseService{ constructor(private readonly _serviceA: ServiceA){ super(_serviceA); } getCat(): string{ return this.runServiceA(); } } 1 2 3 4 5 6 7 8 9 10 11 12 //src/cats/cats.controller.ts import { ServiceB } from \u0026#39;./serviceB\u0026#39;; @Controller(\u0026#39;cats\u0026#39;) export class CatsController{ constructor(private readonly serviceB: ServiceB){} ... @Get(\u0026#39;/serviceB\u0026#39;) getCat(): string{ return this.serviceB.getCat(); } } 1 2 3 4 5 6 7 8 9 10 //src/cats/cats.module.ts import { ServiceB } from \u0026#39;./cats/serviceB\u0026#39;; import { ServiceA } from \u0026#39;./cats/serviceA\u0026#39;; @Module({ imports: [], controllers: [AppController, CatsController], providers: [AppService, CatsService, ServiceB, ServiceA], }) export class AppModule {} Notice that while ServiceB and ServiceA have @Injectable(), BaseService does not. That is because BaseService is never injected directly (it is used as a base class). Notice that @Injectable() allows ServiceB and ServiceA to be managed by the IoC container providers: [AppService, CatsService, ServiceB, ServiceA] associates tokens to corresponding classes: 1 2 3 4 5 | tokens | class | | AppService | AppService | | CatsService | CatsService | | ServiceB | ServiceB | | ServiceA | ServiceA | Whenever IoC container instantiate BaseService, it will check its dependency which is ServiceA and check the lookup table to get its class Whenever IoC container instantiate CatsController, it will check its dependency which is ServiceB and check the lookup table to get its class 1 2 curl http://localhost:3000/cats/serviceB # Hello from Cat A! Provider with Inheritance - @Inject() The same can be done by using Inject() This is more preferable as we don\u0026rsquo;t have to pass in the ServiceA provider to the base class using super() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 //src/cats/convenientBaseService.ts import { Inject } from \u0026#34;@nestjs/common\u0026#34;; import { ServiceA } from \u0026#34;./serviceA\u0026#34;; export class ConvenientBaseService{ @Inject(ServiceA) private readonly serviceA: ServiceA; getCat(): string{ return \u0026#39;Hello from Convenient Base Cat!\u0026#39;; } runServiceA(): string{ return `Convenient version - ${this.serviceA.getCat()}`; } } 1 2 3 4 5 6 7 8 9 10 //src/cats/convenientServiceB.ts import { Injectable } from \u0026#34;@nestjs/common\u0026#34;; import { ConvenientBaseService } from \u0026#34;./convenientBaseService\u0026#34;; @Injectable() export class ConvenientServiceB extends ConvenientBaseService{ getCat(): string{ return this.runServiceA(); } } 1 2 3 4 5 6 7 8 9 10 11 12 //src/cats/cats.controller.ts import { ConvenientServiceB } from \u0026#39;./convenientServiceB\u0026#39;; @Controller(\u0026#39;cats\u0026#39;) export class CatsController{ constructor(private readonly convenientServiceB: ConvenientServiceB){} @Get(\u0026#39;/convenientServiceB\u0026#39;) getConvenientCat(): string { return this.convenientServiceB.getCat(); } } 1 2 3 4 5 6 7 8 9 //src/app.module.ts import { ConvenientServiceB } from \u0026#39;./cats/convenientServiceB\u0026#39;; @Module({ imports: [], controllers: [AppController, CatsController], providers: [AppService, CatsService, ServiceB, ServiceA, ConvenientServiceB], }) export class AppModule {} 1 2 curl http://localhost:3000/cats/convenientServiceB/ # Convenient version - Hello from Cat A! Custom Providers Non-class-based provider tokens Note that the provider tokens can be a string or a symbol Consider another provider: 1 2 3 4 5 6 7 8 9 //src/cats/notClassProvider.ts import { Injectable } from \u0026#34;@nestjs/common\u0026#34;; @Injectable() export class CatOutsideClass{ getOutdoorCat(): string{ return \u0026#39;Does this work with a non-class provider token?\u0026#39;; } } This can be registered to IoC container like so: 1 2 3 4 5 6 7 8 //src/app.module.ts import { CatOutsideClass } from \u0026#39;./cats/notClassProvider\u0026#39;; @Module({ ... providers: [ {provide: \u0026#39;outdoorCat\u0026#39;, useClass: CatOutsideClass}, ] }) To inject such provider, it can be done as the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import { Controller, Get, Post, Body , Inject} from \u0026#39;@nestjs/common\u0026#39;; import { CatOutsideClass } from \u0026#39;./notClassProvider\u0026#39;; @Controller(\u0026#39;cats\u0026#39;) export class CatsController { constructor( ... @Inject(\u0026#39;outdoorCat\u0026#39;) private readonly outdoorCat: CatOutsideClass , ) {} ... @Get(\u0026#39;/outdoor\u0026#39;) getOutdoorCatOutput(): string { return this.outdoorCat.getOutdoorCat(); } } This can be confirmed as shown below: 1 2 curl http://localhost:3000/cats/outdoor # Does this work with a non-class provider token? Class provider - useClass Can dynamically determine a class that a token should resolve to From the above example, useClass was used to assign 'outdoorCat' token to CatOutsideClass class In order to use such provider, we must use @Inject() Value provider - useValue Can be used to inject a constant value Often useful to use mock values for testing The registration step can be done at src/app.module.ts useValue property must have a literal object that has the same interface as the provide object or must be a instance generated by the new keyword There are three examples illustrated below to show how useValue can be used in different scenarios Using constant values 1 2 3 4 5 6 //src/cats/weirdCat.ts export const weirdCatConst = { name: \u0026#34;kaeun\u0026#34;, age: 97, color: \u0026#34;blue\u0026#34; } This can be registered like so: 1 2 3 4 5 6 7 8 9 //src/app.module.ts import { weirdCatConst } from \u0026#39;./cats/weirdCat\u0026#39;; @Module({ ... providers: [ {provide: \u0026#39;weirdCat\u0026#39;, useValue: weirdCatConst}], }) export class AppModule {} This constant value can be injected using @Inject() 1 2 3 4 5 6 7 8 9 10 11 @Controller(\u0026#39;cats\u0026#39;) export class CatsController { constructor( @Inject(\u0026#39;weirdCat\u0026#39;) private readonly catConst ) {} ... @Get(\u0026#39;/getWeirdCat\u0026#39;) getWeirdCat(): string { return this.catConst; } } The dependency injection can be confirmed as the following 1 2 curl http://localhost:3000/cats/getWeirdCat # {\u0026#34;name\u0026#34;:\u0026#34;kaeun\u0026#34;,\u0026#34;age\u0026#34;:97,\u0026#34;color\u0026#34;:\u0026#34;blue\u0026#34;} Using mock values Let us create a mock ServiceA to see how this works 1 2 3 4 5 6 7 8 9 10 11 12 //src/app.module.ts const mockServiceA = { getCat: function mockGetCat(){ return \u0026#39;this is mocking serviceA for testing!\u0026#39;; } } @Module({ ... providers: [ ... {provide: ServiceA, useValue: mockServiceA}] }) notice how the interface of mockServiceA is identical to the interface of class ServiceA in /src/cats/serviceA This mockServiceA will now replace the original serviceA 1 2 curl http://localhost:3000/cats/serviceB # this is mocking serviceA for testing! Using the new keyword Consider the following CatNameService 1 2 3 4 5 6 7 8 9 10 11 12 13 //src/cats/catNameService.ts import { Injectable } from \u0026#34;@nestjs/common\u0026#34;; @Injectable() export class CatNameService { private catName: string = \u0026#39;no cat name\u0026#39;;; setCatName(catName: string){ this.catName = catName; } getCatName(): string{ return `the cat name is ${this.catName}`; } } Let us say that we want to test it with a test input. This can be done with a new keyword at /src/app.module.ts 1 2 3 4 5 6 7 8 9 10 11 12 13 //src/app.module.ts import { CatNameService } from \u0026#39;./cats/catNameService\u0026#39;; ... const testCatNameService = new CatNameService(); testCatNameService.setCatName(\u0026#39;testCatName\u0026#39;); @Module({ ... providers:[ ... {provide: CatNameService, useValue: testCatNameService} ] }) Now, CatNameService can be injected as the following 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 //src/cats/cats.controller.ts import { CatNameService } from \u0026#39;./catNameService\u0026#39;; @Controller(\u0026#39;cats\u0026#39;) export class CatsController { constructor( ... private readonly catNameService: CatNameService ) {} ... @Get(\u0026#39;/getCatName\u0026#39;) getCatName(): string { return this.catNameService.getCatName(); } } As expected, the output will be the following 1 2 curl http://localhost:3000/cats/getCatName # the cat name is testCatName Interestingly enough, when adding (i.e. overwriting) the CatNameService provider as the following: 1 2 3 4 5 6 7 8 9 10 //src/app.module.ts import { CatNameService } from \u0026#39;./cats/catNameService\u0026#39;; @Module({ providers: [ ... {provide: CatNameService, useValue: testCatNameService}, CatNameService, ] }) Rather than testCatNameService, the actual class CatNameService will be used. Therefore, when running the following, the output will change 1 2 curl http://localhost:3000/cats/getCatName # the cat name is no cat name ","date":"2024-02-22T09:19:57+09:00","permalink":"https://k223kim.github.io/p/nestjs-providers/","title":"Nestjs Providers"},{"content":"Disclaimer These examples are based on the nestjs document The resource code for this page is provided here Basic Setting 1 nest new controller_practice This will set up a new nest project with corresponding boilerplate code I have selected npm for the package manager running npm run start:dev will start the server on the localhost when accessing http://localhost:3000, you\u0026rsquo;ll see 1 Hello World! Controllers receive specific requests for the application routing controls which controller receives which request each controller has more than one route different routes can perform different actions nest g resource [name] create a CRUD resource generates NestJS building blocks (modules, service, controller classes), entity class, DTO classes, testing (.spec) files For our example, run nest g resource cats @Controller() Required to define a basic controller can assign an optional route path prefix 1 2 //src/cats/cats.controller.ts @Controller(\u0026#39;cats\u0026#39;) notice that when doing nest g resource cats, it creates src/cats/cats.controller.ts that has a prefix cats curl http://localhost:3000/cats will call the GET method and call findAll() which will output This action returns all cats @Get() HTTP request method decorator this defines an endpoint to fetch resources Nest creates a handler for a specific endpoint for HTTP requests endpoint HTTP request method \u0026amp; route path route path prefix declared by the controller + path specified in the method\u0026rsquo;s decorator 1 2 3 4 5 6 7 8 9 10 //src/cats/cats.controller.ts @Controller(\u0026#39;cats\u0026#39;) export class CatsController { @Get(\u0026#39;breed\u0026#39;) //HTTP request method (GET) is made to this endpoint //route path: http://localhost:3000/cats/breed findAll(): string{ return \u0026#39;This action returns all cats\u0026#39;; } } // Nest will map \u0026#39;GET /cats/breed\u0026#39; to this handler Nest routes the request (GET) to findAll() the function name (findAll()) does not matter! @Req Request object (HTTP request) Access to the request object 1 2 3 4 5 6 7 8 //src/cats/cats.controller.ts import { Req } from \u0026#39;@nestjs/common\u0026#39; import { Request } from \u0026#39;express\u0026#39;; @Get() findAll(@Req() request: Request): string{ return \u0026#39;This action returns all cats\u0026#39;; } @Post() endpoint that creates new records it is called a \u0026lsquo;Post handler\u0026rsquo; Route wildcards 1 2 3 4 5 6 //src/cats/cats.controller.ts @Get(\u0026#39;ab*cd\u0026#39;) //matches abcd, ab_cd, abecd, etc findAll(){ return \u0026#39;This route uses a wildcard\u0026#39;; } @HttpCode(204) Response status code is 200 by default POST requests are 201 Can change the behavior 1 2 3 4 5 6 7 //src/cats/cats.controller.ts import { HttpCode } from \u0026#39;@nestjs/common\u0026#39;; @Post() @HttpCode(204) create(){ return \u0026#39;This action adds a new cat\u0026#39;; } @Header() specify custom response header 1 2 3 4 5 6 7 8 9 //src/cats/cats.controller.ts import { Header } from \u0026#39;@nestjs/common\u0026#39;; @Post() @Header(\u0026#39;Cache-Control\u0026#39;, \u0026#39;none\u0026#39;) create(){ return \u0026#39;This action adds a new cat\u0026#39; } // this addes \u0026#39;Cache-Control: none\u0026#39; to the Header this can be confirmed by the following 1 curl -X POST http://localhost:3000/cats -v this shows the newly added response header 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 * Trying 127.0.0.1:3000... * Connected to localhost (127.0.0.1) port 3000 (#0) \u0026gt; POST /cats HTTP/1.1 \u0026gt; Host: localhost:3000 \u0026gt; User-Agent: curl/7.86.0 \u0026gt; Accept: */* \u0026gt; * Mark bundle as not supporting multiuse \u0026lt; HTTP/1.1 201 Created \u0026lt; X-Powered-By: Express \u0026lt; Cache-Control: none \u0026lt; Content-Type: text/html; charset=utf-8 \u0026lt; Content-Length: 26 \u0026lt; ETag: W/\u0026#34;1a-2akZlhd5h5eyBoEmpkMg7vz8ALY\u0026#34; \u0026lt; Date: Mon, 19 Feb 2024 08:38:27 GMT \u0026lt; Connection: keep-alive \u0026lt; Keep-Alive: timeout=5 \u0026lt; * Connection #0 to host localhost left intact This action adds a new cat% @Redirect(url, status code) redirect can be overwritten by the returned url 1 2 3 4 5 6 7 8 9 10 11 12 //src/cats/cats.controller.ts @Get(\u0026#39;docs\u0026#39;) @Redirect(\u0026#39;https://docs.nestjs.com\u0026#39;, 302) //accessing localhost:3000/cats/docs/ will redirect to //https://docs.nestjs.com getDocs(@Query(\u0026#39;version\u0026#39;) version){ if (version \u0026amp;\u0026amp; version === \u0026#39;5\u0026#39;){ return {url: \u0026#39;https://docs.nestjs.com/v5/\u0026#39;}; //accessing localhost:3000/cats/docs?version=5 will redirec to //https://docs.nest.js.com/v5/ } } notice that the returned values will override any arguments passed to @Redirect() Route parameters (`@Get(\u0026rsquo;:var\u0026rsquo;), @Param(\u0026lsquo;var\u0026rsquo;) can capture dynamic value in the request URL route parameters declared in @Get() can be accessed with @Param() 1 2 3 4 5 6 7 8 9 10 11 //src/cats/cats.controller.ts @Get(\u0026#39;:id\u0026#39;)//dynamic token \u0026#39;id\u0026#39; declared findOne(@Param() params: any): string{ console.log(params.id); return `This action returns a #${params.id} cat`; } //OR findOne(@Param(\u0026#39;id\u0026#39;) params: string): string{ console.log(params); return `The action returns a #${params} cat`; } example with multiple route parameters 1 2 3 4 5 6 7 //src/cats/cats.controller.ts @Get(\u0026#39;:id/:number\u0026#39;) findOne(@Param() params: any): string{ console.log(params.id, params.number); return `This action returns ${params.id} and ${params.number}`; } // we can access this by http://localhost:3000/cats/2/100 but a better practice is to declare each param independently with its corresponding type 1 2 3 4 5 6 7 8 9 10 11 //src/cats/cats.controller.ts @Get(\u0026#39;:id/:number\u0026#39;) findOne( @Param(\u0026#39;id\u0026#39;) id: string, @Param(\u0026#39;number\u0026#39;) number: string, ){ return `id: ${id}, number: ${number}`; } // curl http://localhost:3000/cats/1/100 // results in // id: 1, number: 100 Sub-Domain Routing (@Controller({host:}, @HostParam) let us create another controller by using nest g co CatID 1 2 3 4 5 6 7 8 9 10 11 //default controller //src/app.controller.ts @Controller() export class AppController{ constructor(private readonly appService: AppService){} @Get() getHello(): string{ return \u0026#39;Hello World!\u0026#39; } } 1 2 3 4 5 6 7 8 9 10 // src/cat-id/cat-id.controller.ts (created by nest g co id) import { Controller, HostParam, Get } from \u0026#39;@nestjs/common\u0026#39;; @Controller({host: \u0026#39;:id.api.localhost\u0026#39;}) export class CatIdController { @Get() index(@HostParam(\u0026#39;id\u0026#39;) id: string): string{ return `this is sub routing to ${id}`; } } Notice how CatIdController has a endpoint that is the root route (localhost:3000). Also, notice how AppController also has a endpoint that is a root route (localhost:3000) 1 2 3 4 5 6 //src/app.module.ts @Module({ ... controllers: [AppController, CatIdController] ... }) since the app.module.ts has AppController first, that means that when having the same endpoint, the following will result in the same output (Hello World!) 1 2 curl http://localhost:3000 # Hello World! curl http://kaeun.api.localhost:3000 # Hello World! if we modify the app.module.ts as so, when having the same endpoint, CatIdController will be processed first 1 2 3 4 5 6 //src/app.module.ts @Module({ ... controllers: [CatIdController, AppController] ... }) This means the following 1 2 curl http://kaeun.api.localhost:3000 # this is sub routing to kaeun curl http://localhost:3000 # Hello World! Note When requesting to a host that is not in the host parameter, (e.g. http://kaeun.localhost:3000, this request is being requested to the original domain (i.e. http://localhost:3000) DTO (Data Transfer Object) object that defines how the data will be sent over the network recommended to define DTO schema using Typescript classes 1 2 3 4 5 //src/cats/dto/create-cat.dto.ts export class CreateIDDto{ name: string; id: number; } Request Payloads @Body() can be used for the POST route handler to accept client parameters 1 2 3 4 5 6 7 8 //src/cats/cats.controller.ts import { CreateCatDto } from \u0026#39;./dto/create-cat.dto\u0026#39;; @Post() create(@Body() createCatDto: CreateCatDto) { const {name, id} = createCatDto; return `This action adds a cat with name: ${name} and id: ${id}!` } This results in 1 2 curl -X POST http://localhost:3000/cats -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;Kaeun\u0026#34;, \u0026#34;id\u0026#34;: 97}\u0026#39; # This action adds a cat with name: Kaeun and id: 97! DTO can also be used to retrieve information from a GET request Consider a case where we pass in GET http://localhost:3000/users?offset=0\u0026amp;limit=10 To use these input options in the GET request, we can do the following 1 2 3 4 5 // src/cats/dto/create-cat.dto.ts export class GetCatDto{ offset: number; limit: number; } 1 2 3 4 5 6 7 8 9 //src/cats/cats.controller.ts import {GetCatDto} from \u0026#39;./dto/create-cat.dto\u0026#39;; import { Query } from \u0026#39;@nestjs/common\u0026#39; @Get() getDto(@Query() info: GetCatDto){ const {offset, limit} = info; return `got offset: ${offset}, limit: ${limit}`; } This results in curl http://localhost:3000/cats?limit=10\u0026amp;offset=10, we get the following output 1 2 curl http://localhost:3000/cats?limit=10\u0026amp;offset=10 # got offset: 10, limit: 10 Controller and Module controllers always belong to a module controller must be included within the @Module() decorator for Nest to know that it exists (it will then create an instance of this controller class) 1 2 3 4 //src/app.module.ts @Module({ controllers: [CatIdController, AppController],//add controller here }) DTO naming conventions A colleague of mine has pointed out that the above naming style of DTO can be misleading Consider a case where a POST handler accepts some kind of DTO and returns a different kind of DTO. That being said, using the CreateCatDto can be misleading. The name, CreatCatDto is not clear whether it is being used to create a different DTO or if it is the result of the created DTO To clarify, we can perform the following: 1 2 3 4 5 6 7 8 9 10 11 //src/cats/dto/create-cat.dto.ts export class CreateCatRequest{ name: string; id: number; } export class CreateCatResponse{ name: string; id: number; note: string; } 1 2 3 4 5 6 7 8 9 10 11 12 //src/cats/cats.controller.ts import { CreateCatRequest, CreateCatResponse } from \u0026#39;./dto/create-cat.dto\u0026#39;; ... @Post(\u0026#39;revised\u0026#39;) createRevised(@Body() createCatRequest: CreateCatRequest){ const {name, id} = createCatRequest; const createCatResponse : CreateCatResponse = { name, id, note: \u0026#34;first\u0026#34; }; return `This action adds a cat ${JSON.stringify(createCatResponse)}` } This results in the following output 1 2 curl -X POST http://localhost:3000/cats/revised -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;Kaeun\u0026#34;, \u0026#34;id\u0026#34;: 97}\u0026#39; # This action adds a cat {\u0026#34;name\u0026#34;:\u0026#34;Kaeun\u0026#34;,\u0026#34;id\u0026#34;:97,\u0026#34;note\u0026#34;:\u0026#34;first\u0026#34;} Notice that such DTO naming convention has clarified what has been requested and what is the response of it ","date":"2024-02-19T18:32:43+09:00","permalink":"https://k223kim.github.io/p/nestjs-controllers/","title":"Nestjs Controllers"},{"content":"Definition (By Kyle Simpson) Closure is when a function \u0026ldquo;remember\u0026rdquo; its lexical scope even when the function is executed outside that lexical scope Preserving access to a (private) variable It is not a snapshot of the variable This is an explanation of JavaScript Closure based on the document linked. 1 2 3 4 5 6 7 8 9 10 function makeFunc() { const name = \u0026#34;Mozilla\u0026#34;; function displayName() { console.log(name); } return displayName; } const myFunc = makeFunc();//Nothing myFunc();//\u0026#34;Mozilla\u0026#34; Above is the code example from the document. Couple things to notice:\nmakeFunc returns a reference to a function (it is not yet executed in makeFunc) const myFunc = makeFunc(); is used to invoke makeFunc. Therefore, myFunc reference to the instance of the function displayName This invoked makeFunc referenced my myFunc is invoked at the end (i.e. myFun()) Interestingly enough when displayName was invoked (in myFunc()), displayName maintains a reference to its lexical environment so it has access to name.\nPractical Usage Function factory \u0026hellip; Consequently, you can use a closure anywhere that you might normally use an object with only a single method.\nExample from MDN 1 2 3 4 5 6 7 8 9 10 11 function makeAdder(x) { return function add(y) { return x + y; }; } const add5 = makeAdder(5); const add10 = makeAdder(10); console.log(add5(2)); // 7 console.log(add10(2)); // 12 In the add5\u0026rsquo;s lexical scope, x = 5. In the add10\u0026rsquo;s lexical scope, x = 10. Notice how whenever we call add5, add has access to its\u0026rsquo; x which is 5 and the same thing applies for add10.\nClosure with Modules 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 const makeCounter = function () { let privateCounter = 0; function changeBy(val) { privateCounter += val; } return { increment() { changeBy(1); }, decrement() { changeBy(-1); }, value() { return privateCounter; }, }; }; This is equivalent to the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 const makeCounter = function count() { let privateCounter = 0; function changeBy(val) { privateCounter += val; } return { increment : function increase(){ changeBy(1); }, decrement : function decrease(){ changeBy(-1); }, value : function val(){ return privateCounter; } }; }; makeCounter has three public functions: increment, decrement, and value.\n1 2 3 4 5 6 7 8 9 10 11 12 const counter1 = makeCounter(); const counter2 = makeCounter(); console.log(counter1.value()); // 0. counter1.increment(); counter1.increment(); console.log(counter1.value()); // 2. counter1.decrement(); console.log(counter1.value()); // 1. console.log(counter2.value()); // 0. Notice how counter1 does not have access to privateCounter nor changeBy but can access them by using the public functions. Those public functions have access to privateCounter and changeBy \u0008as they all form closures. (i.e. maintains a reference to its lexical environment)\nClosure scope chain 1 2 3 4 5 6 7 8 9 10 11 function outer() { let getY; { const y = 6; getY = () =\u0026gt; y; } console.log(typeof y); // undefined console.log(getY()); // 6 } outer(); Notice that there exists a inner scope that has the following:\n1 2 3 4 {//inner scope const y = 6; getY = () =\u0026gt; y; } Since const y has been declared from the inner scope, the outer scope does not have access to y. That is why console.log(typeof y); outputs undefined. However, as similar to above examples, we can access the private y by getY. Notice that although getY belongs to the outer scope, in the inner scope, getY is assigned to a reference to a function that returns y.\n1 2 3 4 5 6 {//inner scope const y = 6; getY = function returnY (){ return y; } } That being said, when outer is invoked, the outer scope can have access to y using getY. y can be considered \u0026lsquo;private\u0026rsquo; while getY is \u0026lsquo;public.\u0026rsquo;\nRead/Write private variables (proving that closure does not take \u0026lsquo;snapshots\u0026rsquo; of variables) 1 2 3 4 5 6 // myModule.js let x = 5; export const getX = () =\u0026gt; x; export const setX = (val) =\u0026gt; { x = val; }; 1 2 3 4 5 import { getX, setX } from \u0026#34;./myModule.js\u0026#34;; console.log(getX()); // 5 setX(6); //updates x to 6 console.log(getX()); // 6 Notice while x is only accessible at myModule.js, it can be written and read by using setX and getX. Interestingly enough, if we decide to export x, we can have a separate file that can access x.\n1 2 3 4 5 // myModule.js export let x = 1; export const setX = (val) =\u0026gt; { x = val; }; 1 2 3 4 // closureCreator.js import { x } from \u0026#34;./myModule.js\u0026#34;; export const getX = () =\u0026gt; x; // Close over an imported live binding This getX will have a scope chain as it returns x that is from a different scope.\n1 2 3 4 5 6 import { getX } from \u0026#34;./closureCreator.js\u0026#34;; import { setX } from \u0026#34;./myModule.js\u0026#34;; console.log(getX()); // 1 setX(2); console.log(getX()); // 2 Closure in loops Example 1 (from FrontEndMasters) 1 2 3 4 5 6 7 8 9 10 11 12 for (var i = 1; i \u0026lt;= 3; i++){ console.log(i); setTimeout(function(){ console.log(\u0026#39;i: ${i}\u0026#39;);//closing over i }, i*1000); } // 1 // 2 // 3 // i: 4 // i: 4 // i: 4 Couple things to note.\nCallback function Function passed into another function as an argument Types Synchronous Asynchronous 1 2 3 setTimeout(function(){ console.log(\u0026#39;i: ${i}\u0026#39;); }, i*1000); Above is an example of callback function. Specifically an asynchronous callback. To explain what happens, JavaScript Engine executes code in a stack. When it encounters an asynchronous callback, this is added to a callback queue (or a task queue). Then, whenever the stack is empty, any task in the queue is pulled out and is executed. Below example demonstrates how asynchronous callback work (see herefor details).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 function f1() { console.log(\u0026#39;f1\u0026#39;); } function f2() { console.log(\u0026#39;f2\u0026#39;); } function main() { console.log(\u0026#39;main\u0026#39;); setTimeout(f1, 0); f2(); } function next(){ console.log(\u0026#39;last\u0026#39;); } function next2(){ console.log(\u0026#39;last2\u0026#39;); } main(); next(); next2(); //main //f2 //last //last2 //f1 Notice how setTimeout is executed after all the synchronous parts are executed (i.e. after the stack is empty). That being said, considering our original for loop, we now know that setTimeout will be executed after the for loop. Also, notice how the for loop uses a var i. As per the document, var means the following:\nvar are not local to the loop, i.e. they are in the same scope the for loop is in.\n\u0008This means that var does not belong to the scope created by the for loop iteration. That is why when setTimeout is executed, it has access to the i in the outer scope that has a value of 4 (due to closure). To illustrate this behavior, it can be shown as the following\n1 2 3 4 5 6 7 8 9 10 11 var i = 4;//after the for loop setTimeout(function(){ console.log(\u0026#39;i: ${i}\u0026#39;);//closing over i }, i*1000); setTimeout(function(){ console.log(\u0026#39;i: ${i}\u0026#39;);//closing over i }, i*1000); setTimeout(function(){ console.log(\u0026#39;i: ${i}\u0026#39;);//closing over i }, i*1000); How can we resolve this? This can be resolved by making use of the scope.\nUsing let to create a new scope for each iteration. 1 2 3 4 5 6 7 8 9 10 for (var i = 1; i \u0026lt;= 3; i++){ let j = i; // new scope setTimeout(function(){ console.log(\u0026#39;j: ${j}\u0026#39;);//closing over j }, i*1000); } // j: 1 // j: 2 // j: 3 Notice how j belongs to the inner scope of the for loop. That being said, whenever setTimeout is called asynchronously, it has access to the local variable j that has been updated with i. Also note that this is equivalent to the following:\n1 2 3 4 5 6 7 8 9 for (let i = 1; i \u0026lt;= 3; i++){ setTimeout(function(){ console.log(\u0026#39;j: ${j}\u0026#39;);//closing over j }, i*1000); } // j: 1 // j: 2 // j: 3 Create another scope and pass in the variable (creating a local variable within that scope) 1 2 3 4 5 6 7 8 9 10 11 for (var k = 1; k \u0026lt;= 3; k++){ (function temp(j){ setTimeout(function(){ console.log(`j: ${j}`);//closing over j }, k*1000) })(k); } // j: 1 // j: 2 // j: 3 Notice that in the above code block, temp is executed synchronously with for. Also, temp has its\u0026rsquo; own scope for call in the for loop iteration. For the each scope, the k variable is passed in which will increment from 1 to 3. These k values will be assigned to j and print out accordingly.\n","date":"2024-02-07T13:10:36+09:00","permalink":"https://k223kim.github.io/p/javascript-closure/","title":"JavaScript Closure"},{"content":"Commit Convention Overview 1 2 3 4 5 \u0026lt;type\u0026gt;[optional scope]: \u0026lt;description\u0026gt; [blank] [optional body] [blank] [optional footer(s)] Type fix : bug fix feat : new feature etc build : changes that affect build system/external dependencies chore : updating grunt tasks (no production code change) ci : changes to CI configuration style : changes to the code style only (white-space, format, semic-colons, etc) refactor : not fix nor feat perf : improves performance test : adding/correcting tests docs : documentation change ! after type / scope draws attention (obviously) Footer BREAKING CHANGE breaking API change for any type/scope Signed-off-by Reviewed-by Helped-by Reference-to See-also Examples 1 fix: updated so that the API only takes valid uris 1 2 3 4 5 feat!: constantly checks for new ground truth dataset in the DB and trains the model BREAKING CHANGE: airflow DAG created to run this feature weekly Reviewed by: Kaeun Kim Branch Conventions Main Branches master : production-ready develop : latest development Supporting Branches feature branch off from develop when developing new features merge back to develop to add the new feature in the new release Naming convention: anything other than other branches convention release branch off from develop when develop (almost) reflects the desired state of the new release 1 git checkout -b release-1.1 develop merge back to \u0026hellip; master when it is ready to become a real release 1 2 3 git checkout master git merge --no-f release-1.1 git tag -a 1.1 develop for further development of the next release 1 2 git checkout develop git merge --no-f release-1.1 Naming convention: release-** hotfix branch off from master when it is necessary to act immediately upon an undesired state of a live production version 1 git checkout -b hotfix-1.0.1 master merge back to \u0026hellip; master when the bug fix is done 1 2 3 git checkout master git merge --no-ff hotfix-1.0.1 git tag -a 1.0.1 develop to safeguard the bug fix for the next release 1 2 git checkout develop git merge --no-f release-1.1 Naming convention: hotfix-** References https://github.com/angular/angular/blob/22b96b9/CONTRIBUTING.md#type https://www.conventionalcommits.org/en/v1.0.0/ https://nvie.com/posts/a-successful-git-branching-model/ ","date":"2023-12-04T00:08:07+09:00","permalink":"https://k223kim.github.io/p/git-conventions/","title":"Git Conventions"},{"content":"Stored Procedure Basics 1 2 3 4 CREATE PROCEDURE my_procedure() ([Parameter 1], [Parameter 2]) BEGIN SQL Queries END; Stored procedure is a collection of pre-compiled SQL statements wrapped within a CREATE PROCEDURE statement. Execution of a SQL query consists of the following steps (details): Compiling \u0026amp; Binding → Optimizing → Executing Compiling Parsing, syntax check, semantics check Convert to binary representation (execution plan) Optimizing Choose algorithms to use (optimize the execution plan) Executing Run query and fetch output As stored procedure is pre-compiled (already compiled on the server side(DB); first step of execution is done), it is faster than a MySQL function (this runs on the client; please see here for further details regarding the difference between a procedure and a function). Can be called as shown below: 1 call my_procedure() Stored Procedure Parameters Three modes for MySQL stored procedures: IN, OUT, and INOUT. IN: input parameters OUT: output parameters (returned parameters) INOUT: output parameter that is determined by the input parameter We must be also provide the datatype and its length. Below are some examples: IN parameter 1 2 3 4 CREATE PROCEDURE first_procedure(IN input_parameter varchar(50)) BEGIN ... END; 1 CALL first_procedure(\u0026#39;hello\u0026#39;) OUT parameter 1 2 3 4 CREATE PROCEDURE second_procedure(OUT returned_value int) BEGIN ... END; 1 2 CALL second_procedure(@val) SELECT @val as Output INOUT parameter 1 2 3 4 CREATE PROCEDURE third_procedure(INOUT returned_value int, IN input varchar(10)) BEGIN ... END; 1 2 CALL third_procedure(@out, \u0026#39;kaeun\u0026#39;); SELECT @out as Output; Viewing Stored Procedure 1 SHOW CREATE PROCEDURE my_procedure; Stored Procedure with JSON Let us consider the case where the input parameter is a JSON file. Consider the following json file.\n1 2 3 4 5 { \u0026#39;name\u0026#39;: \u0026#39;Kaeun Kim\u0026#39;, \u0026#39;age\u0026#39;: 27, \u0026#39;pets\u0026#39;: [\u0026#39;tico\u0026#39;, \u0026#39;hope\u0026#39;] } Also, consider a table info that has the following columns: name, age, pet. I want to create a stored procedure to filter out rows that has either the same pet as the input JSON, older than the input age.\n1 2 3 | name | age | pet | |------|-----|-----| | | | | This can be achieved by the following stored procedure my_procedure.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 CREATE PROCEDURE my_procedure(json_info JSON) BEGIN SET @name = JSON_UNQUOTE(JSON_VALUE(json_info, \u0026#39;$.name\u0026#39;)) SET @age = JSON_VALUE(json_info, \u0026#39;$.age\u0026#39;) SET @pets = NULLIF(JSON_UNQUOTE(JSON_EXTRACT(json_info, \u0026#39;$.pets\u0026#39;)), \u0026#39;null\u0026#39;) SELECT DISTINCT info.name as name, info.age as age, info.pet as pet FROM info WHERE (@pets IS NULL OR images.pet MEMBER OF (@pets)) AND (@age IS NULL OR images.age \u0026gt; @age)\tEND; A possible output would be the following:\n1 2 3 4 5 | name | age | pet | |-------|-----|------| | John | 30 | None | | Peter | 28 | tico | | Tom | 29 | hope | Extracting Values from JSON Extract scalar value from JSON 1 JSON_VALUE(json_info, \u0026#39;$.name\u0026#39;) Extract any type of value from JSON 1 JSON_EXTRACT(json_info, \u0026#39;$.pets\u0026#39;) Variables (details) User-defined variables (i.e. session variable see here for details) Specific to the current client connection to the MySQL server. 1 SET @val = 10; Local Variables (Procedure variable) 1 2 3 4 5 6 7 8 9 10 11 DELIMITER // CREATE PROCEDURE my_procedure (input INT) BEGIN DECLARE val INT; SET val = 29; SELECT val; END; // DELIMITER ; Reinitialized to NULL each time the procedure is called References https://www.sqlshack.com/learn-mysql-the-basics-of-mysql-stored-procedures/ https://blog.duveen.me/12 ","date":"2023-11-30T10:12:24+09:00","permalink":"https://k223kim.github.io/p/stored-procedure/","title":"Stored Procedure"},{"content":"FCOS (Fully Convolutional One Stage Object Detection) The aim of this post is to fill the gap between the connectivity of the original paper and the code implementation as much as possible to fully understand the practical aspect of FCOS.\nThe key takeaways and strength of FCOS compared to other detector models are the following:\nIt is not a anchor based detector. It is a anchor-free/proposal free detector. There exists three branches; classification branch, regression branch, and center-ness branch. \u0008Because of that, its loss function is composed of the classification loss, regression loss, and a center-ness loss. FCOS Architecture FCOS Overview 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class GeneralizedRCNN(nn.Module): def forward(self, images, targets=None): images = to_image_list(images) features = self.backbone(images.tensors) proposals, proposal_losses = self.rpn(images, features, targets) if self.roi_heads: x, result, detector_losses = self.roi_heads(features, proposals, targets) else: # RPN-only models don\u0026#39;t have roi_heads x = features result = proposals detector_losses = {} if self.training: losses = {} losses.update(detector_losses) losses.update(proposal_losses) return losses return result FCOS architecture has three components:\nBackbone Input: images Output: features RPN Input : images, features, targets Output : proposals, proposal losses Usage : uses the image features to extract proposals Head Input: features, proposals, targets Output: (detection) result, detector losses Usage: uses the features from the backbone and the proposals from the RPN to get the final detection/segmentation output FCOS Module This is the main component of FCOS RPN that computes the proposals and proposal losses. Notice that to calculate the proposal losses, we need to compute the locations. This will be further discussed below when we talk about anchor-free detectors.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class FCOSModule(torch.nn.Module): \u0026#34;\u0026#34;\u0026#34; Module for FCOS computation. Takes feature maps from the backbone and FCOS outputs and losses. Only Test on FPN now. \u0026#34;\u0026#34;\u0026#34; def __init__(self, cfg, in_channels): super(FCOSModule, self).__init__() head = FCOSHead(cfg, in_channels) box_selector_test = make_fcos_postprocessor(cfg) loss_evaluator = make_fcos_loss_evaluator(cfg) self.head = head self.box_selector_test = box_selector_test self.loss_evaluator = loss_evaluator def forward(self, images, features, targets=None): box_cls, box_regression, centerness = self.head(features) locations = self.compute_locations(features) # computes the locations for each feature level if self.training: return self._forward_train( # calculates the training loss locations, box_cls, box_regression, centerness, targets ) else: return self._forward_test( # selects which box to use locations, box_cls, box_regression, centerness, images.image_sizes ) FCOS Head For clarity, please refer the following figure to understand each component of FCOS Head. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class FCOSHead(torch.nn.Module): def __init__(self, cfg, in_channels): super(FCOSHead, self).__init__() num_classes = cfg.MODEL.FCOS.NUM_CLASSES - 1 self.fpn_strides = cfg.MODEL.FCOS.FPN_STRIDES cls_tower = [] bbox_tower = [] \u0026#34;\u0026#34;\u0026#34; \u0008cls_tower = 4 conv layers bbox_tower = 4 conv layers \u0026#34;\u0026#34;\u0026#34; # initialize the bias for focal loss prior_prob = cfg.MODEL.FCOS.PRIOR_PROB bias_value = -math.log((1 - prior_prob) / prior_prob) torch.nn.init.constant_(self.cls_logits.bias, bias_value) self.scales = nn.ModuleList([Scale(init_value=1.0) for _ in range(5)]) # this refers to def forward(self, x): logits = [] bbox_reg = [] centerness = [] for l, feature in enumerate(x): cls_tower = self.cls_tower(feature) box_tower = self.bbox_tower(feature) logits.append(self.cls_logits(cls_tower)) if self.centerness_on_reg: centerness.append(self.centerness(box_tower)) else: centerness.append(self.centerness(cls_tower)) bbox_pred = self.scales[l](self.bbox_pred(box_tower)) if self.norm_reg_targets: bbox_pred = F.relu(bbox_pred) if self.training: bbox_reg.append(bbox_pred) else: bbox_reg.append(bbox_pred * self.fpn_strides[l]) else: bbox_reg.append(torch.exp(bbox_pred)) return logits, bbox_reg, centerness Notice that cls_tower and bbox_tower refers to the classification branch and regression branch from the original paper. Also, notice that the input x is a list of features on 5 different levels ($P_3$, $P_4$, $P_5$, $P_6$, $P_7$). This means that on each feature level, different bounding box regression predictions, center-ness, and logits are calculated. Note that the regression predictions on each feature level are literary in a feature-level. Additionally, the regression predictions are in the $[l, t, r, b]$ format in order to compute the center-ness later on.\n1 2 3 4 5 6 7 # FCOS/fcos_core/modeling/rpn/fcos/loss.py def compute_centerness_targets(self, reg_targets): left_right = reg_targets[:, [0, 2]] top_bottom = reg_targets[:, [1, 3]] centerness = (left_right.min(dim=-1)[0] / left_right.max(dim=-1)[0]) * \\ (top_bottom.min(dim=-1)[0] / top_bottom.max(dim=-1)[0]) return torch.sqrt(centerness) However, our target bounding boxes are in pixels (e.g. in a coco-format bounding box, it will be $[x, y, w, h]$). That being said, we have to convert the gt bounding box annotations (pixel level) to a feature-level bounding box annotations and convert the bbox annotation to a $[l,t,r,b]$ format.\nOne thing to note is how when we are performing inference of FCOS, we have to multiply the FPN strides to convert the feature-level regression output to a pixel level output. See the below code block to see how FCOSHead multiplies the FPN strides during inference:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def forward(self, x): logits = [] bbox_reg = [] centerness = [] for l, feature in enumerate(x): cls_tower = self.cls_tower(feature) box_tower = self.bbox_tower(feature) logits.append(self.cls_logits(cls_tower)) if self.centerness_on_reg: centerness.append(self.centerness(box_tower)) else: centerness.append(self.centerness(cls_tower)) bbox_pred = self.scales[l](self.bbox_pred(box_tower)) if self.norm_reg_targets: bbox_pred = F.relu(bbox_pred) if self.training: bbox_reg.append(bbox_pred) else: # during inferen bbox_reg.append(bbox_pred * self.fpn_strides[l]) else: bbox_reg.append(torch.exp(bbox_pred)) return logits, bbox_reg, centerness What does it mean to have a anchor-free detector? Anchor boxes can be viewed as pre-defined sliding windows or proposals, which are classified as positive or negative patches, with an extra offsets regression to refine the prediction of bounding box locations. \u0026hellip; (Anchor based detectors) often employ intersection over union (IOU) between anchor boxes and ground-truth boxes to determine the label of an anchor box.\nThe most common example of an anchor-based detector is Faster R-CNN. The following code block shows how Faster R-CNN uses anchors to calculate the (bounding box) regression loss. In order to calculate the regression loss, we have to figure out the regression target from the target bounding boxes and predefined anchors. See below how IOU (Intersection Over Union) is calculated for each target bounding box and anchor pair to get regression target along with their labels (whether it is an object or not/background or foreground).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # simplified code block from FCOS/fcos_core/modeling/rpn/loss.py def match_targets_to_anchors(self, anchor, target, copied_fields=[]): match_quality_matrix = boxlist_iou(target, anchor) matched_idxs = self.proposal_matcher(match_quality_matrix) # RPN doesn\u0026#39;t need any fields from target # for creating the labels, so clear them all target = target.copy_with_fields(copied_fields) # get the targets corresponding GT for each anchor # NB: need to clamp the indices because we can have a single # GT in the image, and matched_idxs can be -2, which goes # out of bounds matched_targets = target[matched_idxs.clamp(min=0)] matched_targets.add_field(\u0026#34;matched_idxs\u0026#34;, matched_idxs) return matched_targets def prepare_targets(self, anchors, targets): labels = [] regression_targets = [] for anchors_per_image, targets_per_image in zip(anchors, targets): # calculates iou between target bounding boxes and anchors # returns targets that matches the anchors matched_targets = self.match_targets_to_anchors( anchors_per_image, targets_per_image, self.copied_fields ) matched_idxs = matched_targets.get_field(\u0026#34;matched_idxs\u0026#34;) labels_per_image = self.generate_labels_func(matched_targets) labels_per_image = labels_per_image.to(dtype=torch.float32) # Background (negative examples) bg_indices = matched_idxs == Matcher.BELOW_LOW_THRESHOLD labels_per_image[bg_indices] = 0 # discard anchors that go out of the boundaries of the image if \u0026#34;not_visibility\u0026#34; in self.discard_cases: labels_per_image[~anchors_per_image.get_field(\u0026#34;visibility\u0026#34;)] = -1 # discard indices that are between thresholds if \u0026#34;between_thresholds\u0026#34; in self.discard_cases: inds_to_discard = matched_idxs == Matcher.BETWEEN_THRESHOLDS labels_per_image[inds_to_discard] = -1 # compute regression targets regression_targets_per_image = self.box_coder.encode( matched_targets.bbox, anchors_per_image.bbox ) labels.append(labels_per_image) regression_targets.append(regression_targets_per_image) return labels, regression_targets On the other hand, FCOS uses the following code block to convert the target bounding boxes to compute the regression target by calculating locations\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def prepare_targets(self, points, targets): object_sizes_of_interest = [ [-1, 64], [64, 128], [128, 256], [256, 512], [512, INF], ] expanded_object_sizes_of_interest = [] for l, points_per_level in enumerate(points): object_sizes_of_interest_per_level = \\ points_per_level.new_tensor(object_sizes_of_interest[l]) expanded_object_sizes_of_interest.append( object_sizes_of_interest_per_level[None].expand(len(points_per_level), -1) ) expanded_object_sizes_of_interest = torch.cat(expanded_object_sizes_of_interest, dim=0) num_points_per_level = [len(points_per_level) for points_per_level in points] self.num_points_per_level = num_points_per_level points_all_level = torch.cat(points, dim=0) labels, reg_targets = self.compute_targets_for_locations( points_all_level, targets, expanded_object_sizes_of_interest ) for i in range(len(labels)): labels[i] = torch.split(labels[i], num_points_per_level, dim=0) reg_targets[i] = torch.split(reg_targets[i], num_points_per_level, dim=0) labels_level_first = [] reg_targets_level_first = [] for level in range(len(points)): labels_level_first.append( torch.cat([labels_per_im[level] for labels_per_im in labels], dim=0) ) reg_targets_per_level = torch.cat([ reg_targets_per_im[level] for reg_targets_per_im in reg_targets ], dim=0) if self.norm_reg_targets: reg_targets_per_level = reg_targets_per_level / self.fpn_strides[level] reg_targets_level_first.append(reg_targets_per_level) return labels_level_first, reg_targets_level_first Personal Notes I got a chance to make use of the FCOS (specifically, CondInst and BoxInst). I\u0026rsquo;ve written what I wanted to note during different experiments and implementations down below.\nAdding new heads In order to use FCOS to calculate something else (other than detection or classification), using the bounding box features (or the classification feature, or maybe both), adding another head can be helpful. That means something like the following (+I realized how a \u0026lsquo;head\u0026rsquo; refers to a single convolutional layer.):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class FCOSHead(torch.nn.Module): def __init__(self, cfg, in_channels): super(FCOSHead, self).__init__() # original heads self.cls_logits = nn.Conv2d( in_channels, num_classes, kernel_size=3, stride=1, padding=1 ) self.bbox_pred = nn.Conv2d( in_channels, 4, kernel_size=3, stride=1, padding=1 ) self.centerness = nn.Conv2d( in_channels, 1, kernel_size=3, stride=1, padding=1 ) # adding another head self.volume_pred = nn.Conv2d(self.in_channels, 1, 3, padding=1) Feature map to pixel space (input image space) For each location $(x, y)$ on the feature map $F_i$ , we can map it back onto the input image as $(\\lfloor \\frac{s}{2}\\rfloor + xs, \\lfloor \\frac{s}{2} \\rfloor+ ys)$ which is near the center of the receptive field of the location $(x, y)$. ($s$ is the total stride until the layer)\nThis means that when the regression output is $[0.75, 0.6, 0.8, 0.8]$ at the feature level where the stride is 128, the actual predicted bounding box area is $(0.75 (left) + 0.8 (right))\\times (0.6(top) + 0.8 (bottom)) \\times 128^2 (stride)$.\nConcatenating features When concatenating input features, (here, I wanted to concatenate the classification features and bounding box regression features to make use of the class and the bounding box information all together) it can be done as the following\n1 2 3 4 5 6 7 8 ctrness_pred = self.ctrness(bbox_tower) reg_pred = self.bbox_pred(bbox_tower) logits_pred = self.cls_logits(cls_tower) reg_pred = F.relu(scale(reg_pred), inplace=True) #concatenate two features two_towers = torch.cat((bbox_tower, cls_tower), dim=1) reg_pred2 = self.another_head(two_towers) reg_pred2 = F.relu(reg_pred2, inplace=True) FCOS code structure breakdown I wanted to understand the original repository\u0026rsquo;s code structure (based on maskrcnn-benchmark\u0026rsquo;s structure). Some comments are added to the code structure.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 . ├── configs # configs for each variation of model architecture ├── demo ├── docker ├── fcos │ ├── __init__.py │ ├── bin │ ├── configs -\u0026gt; ../configs/fcos │ └── fcos.py #main FCOS ├── fcos_core │ ├── README.md │ ├── __init__.py │ ├── config │ │ ├── __init__.py │ │ ├── defaults.py │ │ └── paths_catalog.py │ ├── csrc │ ├── data # handling dataset (e.g. dataset/loader, batch sampler, collate..) │ │ ├── __init__.py │ │ ├── build.py │ │ ├── collate_batch.py │ │ ├── datasets │ │ ├── samplers │ │ └── transforms │ ├── engine │ │ ├── __init__.py │ │ ├── bbox_aug.py │ │ ├── inference.py │ │ └── trainer.py │ ├── layers │ │ ├── __init__.py │ │ ├── _utils.py │ │ ├── batch_norm.py │ │ ├── dcn │ │ ├── iou_loss.py │ │ ├── misc.py │ │ ├── nms.py │ │ ├── roi_align.py │ │ ├── roi_pool.py │ │ ├── scale.py │ │ ├── sigmoid_focal_loss.py │ │ └── smooth_l1_loss.py │ ├── modeling # different components of the architecture │ │ ├── __init__.py │ │ ├── backbone # different backbones to extract features │ │ ├── detector │ │ │ ├── __init__.py │ │ │ ├── detectors.py │ │ │ └── generalized_rcnn.py │ │ ├── roi_heads # different heads │ │ │ ├── __init__.py │ │ │ ├── box_head │ │ │ ├── keypoint_head │ │ │ ├── mask_head │ │ │ └── roi_heads.py │ │ ├── rpn # different Region Proposal Networks │ │ │ ├── __init__.py │ │ │ ├── anchor_generator.py │ │ │ ├── fcos │ │ │ │ ├── __init__.py │ │ │ │ ├── fcos.py # has FCOS module and head │ │ │ │ ├── inference.py │ │ │ │ └── loss.py │ │ │ ├── inference.py │ │ │ ├── loss.py │ │ │ ├── retinanet │ │ │ │ ├── __init__.py │ │ │ │ ├── inference.py │ │ │ │ ├── loss.py │ │ │ │ └── retinanet.py │ │ │ ├── rpn.py # this has build_rpn() to call different rpns │ │ │ └── utils.py │ │ └── utils.py │ ├── solver │ ├── structures │ └── utils ├── onnx ├── requirements.txt ├── setup.py ├── tests └── tools ","date":"2023-11-17T14:31:50+09:00","permalink":"https://k223kim.github.io/p/fcos/","title":"FCOS"},{"content":" View process and GPU status 1 nvidia-smi Information of the process 1 ps -ef | grep ${PID number} ","date":"2023-10-25T23:42:52+09:00","permalink":"https://k223kim.github.io/p/linux-commands/","title":"Linux Commands"},{"content":"I often use coco format detection/segmentation dataset and want to note some useful commands. The library used for coco dataset is called pycocotools. Below are some common usage of pycocotools (for me at least).\n1 from pycocotools import coco Useage Loading coco dataset 1 2 coco_path = \u0026#39;/path/to/coco.json\u0026#39; coco_info = coco.COCO(coco_path) Loading images and appropriate annotations 1 2 3 imgs = coco_info.loadImgs(coco_info.getImgIds()) #all images in the dataset for img in imgs: anns = coco_info.loadAnns(coco_info.getAnnIds(imgIds=img[\u0026#39;id\u0026#39;])) Loading annotations based on categories 1 2 3 4 category_names = [\u0026#39;cls1\u0026#39;, \u0026#39;cls2\u0026#39;, \u0026#39;cls3\u0026#39;, \u0026#39;cls4\u0026#39;] thsc_ids = coco_info.getCatIds(catNms=category_names) ann_ids = coco_info.getAnnIds(catIds=thsc_ids) ann_info = coco_info.loadAnns(ann_ids) Overwriting the coco dataset and save to json 1 2 3 coco_info.dataset[\u0026#39;images\u0026#39;] = new_images # list of new images with open(\u0026#39;/path/to/new/coco.json\u0026#39;, \u0026#39;w+\u0026#39;) as f: json.dump(coco_info.dataset, f) ","date":"2023-10-25T21:57:42+09:00","permalink":"https://k223kim.github.io/p/coco-dataset-and-pycocotools/","title":"COCO dataset and pycocotools"},{"content":"Often times, you may have multiple github accounts in one Mac. Below are the steps to add ssh-key for multiple github accounts.\nGenerate SSH keys You must generate ssh keys for each github account. Make sure to have unique names for each key file. Notice that the email used for the ssh key generation is irrelevant to the github account. (In fact, as per this source, email is just a comment?)\n1 ssh-keygen -t rsa -b 4096 -C \u0026#34;your_email@example.com\u0026#34; When prompted: Enter file in which to save the key, make sure to modify the name for each account (e.g. id_rsa_kaeun).\nAdd SSH key to Github Account Go to Settings/SSH and GPG Keys in your Github Account and add copy and paste the output of the following command.\n1 cat ~/.ssh/\u0026lt;ssh-key file name\u0026gt;.pub Modify ~/.ssh/config Add the following to your ~/.ssh/config file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 #user1 account Host github.com-user1 HostName github.com User git IdentityFile ~/.ssh/github-user1 IdentitiesOnly yes #user2 account Host github.com-user2 HostName github.com User git IdentityFile ~/.ssh/github-user2 IdentitiesOnly yes SSH agent 1 2 3 # ssh-add \u0026lt;IdentityFile\u0026gt; ssh-add **~/.ssh/${name of ssh key file}** # Use **eval \u0026#34;$(ssh-agent -s)\u0026#34;** if necessary Clone When cloning repositories run the following command.\n1 git clone git@${Host}:${user_name}/${your-repo-name}.git Testing SSH connection 1 ssh -T git@github.com Delete recently pushed commit 1 2 git reset HEAD^ git push -f origin ${branch_name} Add git user name locally 1 2 3 # in that local directory git config user.name ${username} git config user.email ${useremail} How to connect a remote repository to a local directory 1 2 3 4 5 6 7 8 git init git remote add origin git@github.com:${username}/${repository_name}.git git fetch origin git branch --set-upstream-to=origin/main # can now add, commit, and push git add . git commit -m \u0026#34;your commit message\u0026#34; git push origin main References https://gist.github.com/Jonalogy/54091c98946cfe4f8cdab2bea79430f9 ","date":"2023-10-25T18:03:00+09:00","image":"https://k223kim.github.io/p/github-ssh-set-up/GitHub-Mark_hu9d76232819f01fb0167220a8ea045d21_7249_120x120_fill_box_smart1_3.png","permalink":"https://k223kim.github.io/p/github-ssh-set-up/","title":"Github Ssh Set Up"}]